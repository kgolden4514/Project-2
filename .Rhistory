n.trees = c( 100, 500, 1000) ,
shrinkage = 0.1,
n.minobsinnode = 10)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = lifestyle_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,10, 20),
n.trees = c( 100, 500, 1000) ,
shrinkage = 0.1,
n.minobsinnode = 10)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = lifestyle_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
lifestyle_train %>% summarise(var(shares))
lifestyle_train %>% summarise(sd(shares))
names(online)
names(lifestyle_train)
# Create correlation graph
correlation_graph <- function(data_input,sig=0.5){
corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
corr <- subset(corr, abs(value) > 0.5)
corr[order(-abs(corr$value)),]
print(corr)
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
correlation_graph(lifestyle_train)
#Create correlation table and graph for a training dataset
correlation_table <- function(data_input) {
#drop binary variables
correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
kable(correlations, caption = 'Correlations Lifestyle')
}
correlation_table(lifestyle_train)
correlation_table(lifestyle_train)
as_tibble(correlation_table(lifestyle_train))
## creates a data frame of the four models RMSE on the
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
#linear_2_RMSE=linear_2_RMSE[1],
#random_forest_RMSE=random_forest_RMSE[1]
boosted_tree_RMSE = boosted_tree_RMSE[1] )
models_RMSE
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,10, 20),
n.trees = c( 100, 500, 1000) ,
shrinkage = 0.1,
n.minobsinnode = 10)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = lifestyle_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = lifestyle_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)
## creates boosted tree model with tuning parameters n.trees, shrinkage, interaction.depth from CV
boosted_tree_model <- gbm(shares ~ ., data = lifestyle_train,
distribution = "gaussian",
n.trees = 500 ,
shrinkage = 0.1 ,
interaction.depth = 6 )
## test set prediction
boosted_tree_model_pred <- predict(boosted_tree_model, newdata = dplyr::select(lifestyle_test, -shares), n.trees = 500)
## stores results
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)
## creates a data frame of the four models RMSE on the
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
#linear_2_RMSE=linear_2_RMSE[1],
#random_forest_RMSE=random_forest_RMSE[1]
boosted_tree_RMSE = boosted_tree_RMSE[1] )
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = lifestyle_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)
linear_1_RMSE
boosted_tree_RMSE
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = lifestyle_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = lifestyle_test$shares)
## creates boosted tree model with tuning parameters n.trees, shrinkage, interaction.depth from CV
boosted_tree_model <- gbm(shares ~ ., data = lifestyle_train,
distribution = "gaussian",
n.trees = 500 ,
shrinkage = 0.1 ,
interaction.depth = 6 )
## test set prediction
boosted_tree_model_pred <- predict(boosted_tree_model, newdata = dplyr::select(lifestyle_test, -shares), n.trees = 500)
## stores results
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)
## creates a data frame of the four models RMSE on the
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
#linear_2_RMSE=linear_2_RMSE[1],
#random_forest_RMSE=random_forest_RMSE[1]
boosted_tree_RMSE = boosted_tree_RMSE[1] )
## gets the name of the column with the smallest rmse
smallest_RMSE<-colnames(models_RMSE)[apply(models_RMSE,1,which.min)]
## declares the model with smallest RSME the winner
paste0(" For ",
## parameter will be lifestyle for this one I think
#parameter,
smallest_RMSE, " is the winner")
models_RMSE
linear_1_RMSE
lifestyle_train %>% head()
cor(lifestyle_train)
cor(lifestyle_train)[1,]
cor(lifestyle_train)[1,] %>% sort()
cor(entertainment_train)[1,] %>% sort()
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
entertainment <- online[ , -c(12, 14:17)]
#Filter out the zeros from the remaining data_channel_is column
entertainment <- entertainment %>%
filter(Entertainment == 1)
#Drop the data_channel_is column
entertainment <- entertainment[ , -c(12)]
entertainment <- entertainment[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(entertainment), size = nrow(entertainment)*.70)
test <- setdiff(1:nrow(entertainment), train)
# trainiing and testing subsets
entertainment_train <- entertainment[train, ]
entertainment_test <- entertainment[test, ]
cor(entertainment_train)[1,] %>% sort()
names(lifestyle_train)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = lifestyle_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = entertainment_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = lifestyle_test$shares)
linear_1_RMSE
# #Create data_channel_is_tech dataset
# #Take out all data_Channel_is columns that are not relevant to this data set
tech <- online[ , -c(12:15, 17)]
#
# #Filter out the zeros from the remaining data_channel_is column
tech <- tech %>%
filter(data_channel_is_tech == 1)
# #Create data_channel_is_tech dataset
# #Take out all data_Channel_is columns that are not relevant to this data set
tech <- online[ , -c(12:15, 17)]
#
# #Filter out the zeros from the remaining data_channel_is column
tech <- tech %>%
filter(Tech == 1)
#
# #Drop the data_channel_is column
tech <- tech[ , -c(12)]
tech <- tech[ , c(53, 1:52)]
#
# #Split the data into training and test
set.seed(5432)
#
# # Split the data into a training and test set (70/30 split)
# # indices
#
train <- sample(1:nrow(tech), size = nrow(tech)*.70)
test <- setdiff(1:nrow(tech), train)
#
# # trainiing and testing subsets
tech_train <- tech[train, ]
tech_test <- tech[test, ]
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = tech_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = lifestyle_test$shares)
linear_1_RMSE
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = tech_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7, 10),
n.trees = c(1, 5, 10)*10 ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1, 5, 10)*10 ,
shrinkage = 0.1,
n.minobsinnode = 20)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = 20)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = entertainment_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(entertainment_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = lifestyle_test$shares)
linear_1_RMSE
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = entertainment_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(entertainment_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = entertainment_test$shares)
linear_1_RMSE
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = 20)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
gmb_tree_cv
gbm_tree_cv
## creates boosted tree model with tuning parameters n.trees, shrinkage, interaction.depth from CV
boosted_tree_model <- gbm(shares ~ ., data = entertainment_train,
distribution = "gaussian",
n.trees = 16 ,
shrinkage = 0.1 ,
interaction.depth = 7 )
## test set prediction
boosted_tree_model_pred <- predict(boosted_tree_model, newdata = dplyr::select(entertainment_test, -shares), n.trees = 7)
## stores results
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = entertainment_test$shares)
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = entertainment_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(entertainment_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = entertainment_test$shares)
## creates a data frame of the four models RMSE on the
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
#linear_2_RMSE=linear_2_RMSE[1],
#random_forest_RMSE=random_forest_RMSE[1]
boosted_tree_RMSE = boosted_tree_RMSE[1] )
## gets the name of the column with the smallest rmse
smallest_RMSE<-colnames(models_RMSE)[apply(models_RMSE,1,which.min)]
## declares the model with smallest RSME the winner
paste0(" For ",
## parameter will be lifestyle for this one I think
#parameter,
smallest_RMSE, " is the winner")
models_RMSE
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(20, 40))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20)*10 ,
shrinkage = 0.1,
n.minobsinnode = c(20, 40))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
cor(entertainment_train)[1,] %>% sort()
## delete## exploritory to try to figure out which variables to include in models cause weird results##
cor(entertainment_train)[1,] %>% sort()
cor(lifestyle_train)[1,] %>% sort()
