train <- sample(1:nrow(world), size = nrow(world)*.70)
test <- setdiff(1:nrow(world), train)
# trainiing and testing subsets
world_train <- world[train, ]
world_test <- world[test, ]
#Creates Summary Tables for a training dataset
tables <- function(data_input) {
d <- describe(lifestyle_train[ , c('shares')], fast=TRUE)
kable(d, caption = 'Shares Summary')
}
#Create correlation table and graph for a training dataset
correlation.table <- function(data_input) {
#drop binary variables
correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
kable(correlations, caption = 'Correlations Lifestyle')
}
# Create correlation graph
corr_simple <- function(data_input,sig=0.5){
corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
corr <- subset(corr, abs(value) > 0.5)
corr[order(-abs(corr$value)),]
print(corr)
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
# #Create scatterplots
# scatterplots <- function(data_input, xwert, y = shares) {
#   g <- ggplot(data_input, aes(x = xwert, y = shares))
#   g + geom_point(size = 2)
# }
# scatterplots(data_input = lifestyle_train, x = n_unique_tokens)
# ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()
# # Change the point size, and shape
# ggplot(mtcars, aes(x=wt, y=mpg)) +
#   geom_point(size=2, shape=23)
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)]
#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
filter(data_channel_is_lifestyle == 1)
#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)]
lifestyle <- lifestyle[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)
# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
#Shares table for lifestyle_train
tables(lifestyle_train)
#Correlation table for lifestyle_train
correlation.table(lifestyle_train)
#Correlation graph for lifestyle_train
corr_simple(lifestyle_train)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
#Dropped url and timedelta because they are non-predictive.
online <- online[ , c(3:61)]
#All trained data sets are in another .Rmd file called Create_dataframes_use_later. These can be copy and pasted when they are necessary
#Creates Summary Tables for a training dataset
tables <- function(data_input) {
d <- describe(lifestyle_train[ , c('shares')], fast=TRUE)
kable(d, caption = 'Shares Summary')
}
#Create correlation table and graph for a training dataset
correlation.table <- function(data_input) {
#drop binary variables
correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
kable(correlations, caption = 'Correlations Lifestyle')
}
# Create correlation graph
corr_simple <- function(data_input,sig=0.5){
corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
corr <- subset(corr, abs(value) > 0.5)
corr[order(-abs(corr$value)),]
print(corr)
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
# Still working this one out
# Create scatterplots
# scatterplots <- function(data_input, xwert, y = shares) {
#   g <- ggplot(data_input, aes(x = xwert, y = shares))
#   g + geom_point(size = 2)
# }
# scatterplots(data_input = lifestyle_train, x = n_unique_tokens)
# ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()
# # Change the point size, and shape
# ggplot(mtcars, aes(x=wt, y=mpg)) +
#   geom_point(size=2, shape=23)
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)]
#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
filter(data_channel_is_lifestyle == 1)
#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)]
lifestyle <- lifestyle[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)
# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
#Shares table for lifestyle_train
tables(lifestyle_train)
#Correlation table for lifestyle_train
correlation.table(lifestyle_train)
#Correlation graph for lifestyle_train
corr_simple(lifestyle_train)
scatterplots <- function(data_input) {
ggplot(data_input, aes(x = kw_min_min, y = shares)) +
geom_point()
}
scatterplots(lifestyle_train)
scatterplots <- function(data_input, x) {
ggplot(data = data_input, aes(x = x, y = shares)) +
geom_point()
}
scatterplots(lifestyle_train, kw_min_min)
scatterplots <- function(data_input, x) {
data_input %>%
ggplot(aes(x = x, y = shares)) + geom_point()
}
# Still working this one out
# Create scatterplots
scatterplots <- function(data_input, x) {
data_input %>%
ggplot(aes(x = x, y = shares)) + geom_point()
}
scatterplots(lifestyle_train, kw_min_min)
scatterplots <- function(data_input, xval) {
data_input %>%
ggplot(aes(x = xval, y = shares)) + geom_point()
}
scatterplots(lifestyle_train, kw_min_min)
scatterplots <- function(data_input) {
data_input %>%
ggplot(aes(x = kw_min_min, y = shares)) + geom_point()
}
scatterplots(lifestyle_train)
install.packages('cowplot')
library(cowplot)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("~/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2")
setwd("~/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents")
setwd("~/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
setwd('C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2')
online <- read.csv('OnlineNewsPopularity.csv')
setwd('C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2/')
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
setwd("~/GitHub/ST_558_Project_2")
setwd("C:/Documents/Github")
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
setwd("C:/Documents/Github")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
setwd("C:/Documents/Github/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
setwd("C:/Documents/Github/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
colnames(online) <- c('url', 'days', 'n.Title', 'n.Content', 'Rate.Unique',
'Rate.Nonstop', 'Rate.Unique.Nonstop', 'n.Links',
'n.Other', 'n.Images', 'n.Videos',
'Avg.Words', 'n.Key', 'Lifestyle', 'Entertainment',
'Business', 'Social.Media', 'Tech', 'World', 'Min.Worst.Key',
'Max.Worst.Key', 'Avg.Worst.Key', 'Min.Best.Key',
'Max.Best.Key', 'Avg.Best.Key', 'Avg.Min.Key', 'Avg.Max.Key',
'Avg.Avg.Key', 'Min.Ref', 'Max.Ref', 'Avg.Ref', 'Mon',
'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun', 'Weekend',
'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04',
'Global.Subj', 'Global.Pol', 'Global.Pos.Rate',
'Global.Neg.Rate', 'Rate.Pos', 'Rate.Neg', 'Avg.Pos.Pol',
'Min.Pos.Pol', 'Max.Pos.Pol', 'Avg.Neg.Pol', 'Min.Neg.Pol',
'Max.Neg.Pol', 'Title.Subj', 'Title.Pol', 'Abs.Subj',
'Abs.Pol', 'shares')
#Dropped url and timedelta because they are non-predictive.
online <- online[ , c(3:61)]
#All trained data sets are in another .Rmd file called Create_dataframes_use_later. These can be copy and pasted when they are necessary
## I'm not 100% sure but I think we are supposed to run this same process with the different params set up. so its like just one doc that we knit with the six different params and it goes through and does everything without us needing to copy and paste
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)]
#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
filter(Lifestyle == 1)
#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)]
lifestyle <- lifestyle[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)
# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
fit_control <- trainControl(method='repeatedcv', number=3, repeats=1)
forest <- train(shares ~.,
data = lifestyle_train,
method ='rf',
ntree = 200,
trControl = fit_control)
set.seed(123)
fit_control <- trainControl(method='repeatedcv', number=3, repeats=1)
forest <- train(shares ~.,
data = lifestyle_train,
method ='rf',
ntree = 200,
importance = TRUE,
trControl = fit_control)
rfFit <- randomForest(price ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
forest$importance
forest$finalModel$importance
e <- as.data.frame(forest$finalModel$importance)
(varImpPlot(e$%IncMSE))
e <- e %>% select(c('%IncMSE'))
(varImpPlot(e))
(varImpPlot(forest))
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
attributes(rfFit)
attributes(forest)
as.tibble(rfFit$importance)
View(e)
varImpPlot(rfFit)
varImpPlot(rfFit,scale=TRUE,n.var=18)
varImpPlot(rfFit,scale=TRUE,n.var=10)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.tibble(rfFit$importance)
varImpPlot(rfFit,scale=TRUE,n.var=10)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE,n.var=10)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE,n.var=10)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
set.seed(10210526)
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
set.seed(10210526)
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
e <- as.data.frame(rfFit$importance)
e <- e %>%
select(c('%IncMSE'))
max(e)
View(e)
rfFit
#Cross validation not needed on random forest
set.seed(10210526)
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/5,
ntree = 200, importance = TRUE)
rfFit
e <- as.data.frame(rfFit$importance)
e
e <- e %>%
select(c('%IncMSE'))
max(e)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Principal Component Analysis
dimensions <- dim(lifestyle_train)
n_rows <- dimensions[1]
n_columns <- dimensions[2]
cat("Number of rows:", n_rows, "\n")
cat("Number of columns:", n_columns, "\n")
colSums(is.na(lifestyle_train))
install.packages("corrr")
library(corrr)
data_normalized <- scale(lifestyle_train)
corr_matrix <- cor(data_normalized)
corr_matrix
install.packages('ggcorrplot')
library(ggcorrplot)
ggcorrplot(corr_matrix)
install.packages("FactoMineR")
install.packages("factoextra")
library(FactoMineR)
library(factoextra)
data.pca <- princomp(corr_matrix)
summary(data.pca)
cor(lifestyle_train)[1,] %>% sort()
#Removed rate.nonstop because they were all 1. Removed Mon-Sat and Weekend because they were binary/categorical
lifestyle_train2 <- lifestyle_train %>%
select(-c('Rate.Nonstop',
'Mon', 'Tues',
'Wed', 'Thurs',
'Fri', 'Sat',
'Sun',
'Weekend'))
#Cross validation not needed on random forest
set.seed(10210526)
#Removed rate.nonstop because they were all 1. Removed Mon-Sat and Weekend because they were binary/categorical
lifestyle_train2 <- lifestyle_train %>%
select(-c('Rate.Nonstop',
'Mon', 'Tues',
'Wed', 'Thurs',
'Fri', 'Sat',
'Sun',
'Weekend'))
rfFit <- randomForest(shares ~ ., data = lifestyle_train2,
mtry = ncol(lifestyle_train2)/3,
ntree = 200, importance = TRUE)
rfFit
e <- as.data.frame(rfFit$importance)
e
e <- e %>%
select(c('%IncMSE'))
max(e)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Please look over this. What do you think? There are far too many variables, but I am not sure which ones to get rid of that I haven't already.
