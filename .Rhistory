<<<<<<< HEAD
n.trees = c( 100, 500, 1000) ,
shrinkage = 0.1,
n.minobsinnode = 10)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = lifestyle_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,10, 20),
n.trees = c( 100, 500, 1000) ,
shrinkage = 0.1,
n.minobsinnode = 10)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = lifestyle_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
lifestyle_train %>% summarise(var(shares))
lifestyle_train %>% summarise(sd(shares))
names(online)
names(lifestyle_train)
# Create correlation graph
correlation_graph <- function(data_input,sig=0.5){
=======
train <- sample(1:nrow(world), size = nrow(world)*.70)
test <- setdiff(1:nrow(world), train)
# trainiing and testing subsets
world_train <- world[train, ]
world_test <- world[test, ]
#Creates Summary Tables for a training dataset
tables <- function(data_input) {
d <- describe(lifestyle_train[ , c('shares')], fast=TRUE)
kable(d, caption = 'Shares Summary')
}
#Create correlation table and graph for a training dataset
correlation.table <- function(data_input) {
#drop binary variables
correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
kable(correlations, caption = 'Correlations Lifestyle')
}
# Create correlation graph
corr_simple <- function(data_input,sig=0.5){
>>>>>>> 77f97f48eab0c5dd7ee97ba3f7b1e6cbc5a470b8
corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
corr <- subset(corr, abs(value) > 0.5)
corr[order(-abs(corr$value)),]
print(corr)
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
<<<<<<< HEAD
correlation_graph(lifestyle_train)
#Create correlation table and graph for a training dataset
correlation_table <- function(data_input) {
#drop binary variables
correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
kable(correlations, caption = 'Correlations Lifestyle')
}
correlation_table(lifestyle_train)
correlation_table(lifestyle_train)
as_tibble(correlation_table(lifestyle_train))
## creates a data frame of the four models RMSE on the
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
#linear_2_RMSE=linear_2_RMSE[1],
#random_forest_RMSE=random_forest_RMSE[1]
boosted_tree_RMSE = boosted_tree_RMSE[1] )
models_RMSE
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,10, 20),
n.trees = c( 100, 500, 1000) ,
shrinkage = 0.1,
n.minobsinnode = 10)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = lifestyle_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = lifestyle_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)
## creates boosted tree model with tuning parameters n.trees, shrinkage, interaction.depth from CV
boosted_tree_model <- gbm(shares ~ ., data = lifestyle_train,
distribution = "gaussian",
n.trees = 500 ,
shrinkage = 0.1 ,
interaction.depth = 6 )
## test set prediction
boosted_tree_model_pred <- predict(boosted_tree_model, newdata = dplyr::select(lifestyle_test, -shares), n.trees = 500)
## stores results
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)
## creates a data frame of the four models RMSE on the
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
#linear_2_RMSE=linear_2_RMSE[1],
#random_forest_RMSE=random_forest_RMSE[1]
boosted_tree_RMSE = boosted_tree_RMSE[1] )
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = lifestyle_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)
linear_1_RMSE
boosted_tree_RMSE
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = lifestyle_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = lifestyle_test$shares)
## creates boosted tree model with tuning parameters n.trees, shrinkage, interaction.depth from CV
boosted_tree_model <- gbm(shares ~ ., data = lifestyle_train,
distribution = "gaussian",
n.trees = 500 ,
shrinkage = 0.1 ,
interaction.depth = 6 )
## test set prediction
boosted_tree_model_pred <- predict(boosted_tree_model, newdata = dplyr::select(lifestyle_test, -shares), n.trees = 500)
## stores results
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)
## creates a data frame of the four models RMSE on the
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
#linear_2_RMSE=linear_2_RMSE[1],
#random_forest_RMSE=random_forest_RMSE[1]
boosted_tree_RMSE = boosted_tree_RMSE[1] )
## gets the name of the column with the smallest rmse
smallest_RMSE<-colnames(models_RMSE)[apply(models_RMSE,1,which.min)]
## declares the model with smallest RSME the winner
paste0(" For ",
## parameter will be lifestyle for this one I think
#parameter,
smallest_RMSE, " is the winner")
models_RMSE
linear_1_RMSE
lifestyle_train %>% head()
cor(lifestyle_train)
cor(lifestyle_train)[1,]
cor(lifestyle_train)[1,] %>% sort()
cor(entertainment_train)[1,] %>% sort()
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
entertainment <- online[ , -c(12, 14:17)]
#Filter out the zeros from the remaining data_channel_is column
entertainment <- entertainment %>%
filter(Entertainment == 1)
#Drop the data_channel_is column
entertainment <- entertainment[ , -c(12)]
entertainment <- entertainment[ , c(53, 1:52)]
=======
# #Create scatterplots
# scatterplots <- function(data_input, xwert, y = shares) {
#   g <- ggplot(data_input, aes(x = xwert, y = shares))
#   g + geom_point(size = 2)
# }
# scatterplots(data_input = lifestyle_train, x = n_unique_tokens)
# ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()
# # Change the point size, and shape
# ggplot(mtcars, aes(x=wt, y=mpg)) +
#   geom_point(size=2, shape=23)
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)]
#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
filter(data_channel_is_lifestyle == 1)
#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)]
lifestyle <- lifestyle[ , c(53, 1:52)]
>>>>>>> 77f97f48eab0c5dd7ee97ba3f7b1e6cbc5a470b8
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
<<<<<<< HEAD
train <- sample(1:nrow(entertainment), size = nrow(entertainment)*.70)
test <- setdiff(1:nrow(entertainment), train)
# trainiing and testing subsets
entertainment_train <- entertainment[train, ]
entertainment_test <- entertainment[test, ]
cor(entertainment_train)[1,] %>% sort()
names(lifestyle_train)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = lifestyle_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = entertainment_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = lifestyle_test$shares)
linear_1_RMSE
# #Create data_channel_is_tech dataset
# #Take out all data_Channel_is columns that are not relevant to this data set
tech <- online[ , -c(12:15, 17)]
#
# #Filter out the zeros from the remaining data_channel_is column
tech <- tech %>%
filter(data_channel_is_tech == 1)
# #Create data_channel_is_tech dataset
# #Take out all data_Channel_is columns that are not relevant to this data set
tech <- online[ , -c(12:15, 17)]
#
# #Filter out the zeros from the remaining data_channel_is column
tech <- tech %>%
filter(Tech == 1)
#
# #Drop the data_channel_is column
tech <- tech[ , -c(12)]
tech <- tech[ , c(53, 1:52)]
#
# #Split the data into training and test
set.seed(5432)
#
# # Split the data into a training and test set (70/30 split)
# # indices
#
train <- sample(1:nrow(tech), size = nrow(tech)*.70)
test <- setdiff(1:nrow(tech), train)
#
# # trainiing and testing subsets
tech_train <- tech[train, ]
tech_test <- tech[test, ]
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = tech_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = lifestyle_test$shares)
linear_1_RMSE
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = tech_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7, 10),
n.trees = c(1, 5, 10)*10 ,
shrinkage = 0.1,
n.minobsinnode = c(10,20))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1, 5, 10)*10 ,
shrinkage = 0.1,
n.minobsinnode = 20)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = 20)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = entertainment_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(entertainment_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = lifestyle_test$shares)
linear_1_RMSE
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = entertainment_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(entertainment_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = entertainment_test$shares)
linear_1_RMSE
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = 20)
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
gmb_tree_cv
gbm_tree_cv
## creates boosted tree model with tuning parameters n.trees, shrinkage, interaction.depth from CV
boosted_tree_model <- gbm(shares ~ ., data = entertainment_train,
distribution = "gaussian",
n.trees = 16 ,
shrinkage = 0.1 ,
interaction.depth = 7 )
## test set prediction
boosted_tree_model_pred <- predict(boosted_tree_model, newdata = dplyr::select(entertainment_test, -shares), n.trees = 7)
## stores results
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = entertainment_test$shares)
## linear regression model using all predictors
linear_model_1 <- train( shares ~ ., data = entertainment_train,
method = "lm",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 5))
## prediction of test with model
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(entertainment_test, -shares))
## storing error of model on test set
linear_1_RMSE<- postResample(linear_model_1_pred, obs = entertainment_test$shares)
## creates a data frame of the four models RMSE on the
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
#linear_2_RMSE=linear_2_RMSE[1],
#random_forest_RMSE=random_forest_RMSE[1]
boosted_tree_RMSE = boosted_tree_RMSE[1] )
## gets the name of the column with the smallest rmse
smallest_RMSE<-colnames(models_RMSE)[apply(models_RMSE,1,which.min)]
## declares the model with smallest RSME the winner
paste0(" For ",
## parameter will be lifestyle for this one I think
#parameter,
smallest_RMSE, " is the winner")
models_RMSE
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20) ,
shrinkage = 0.1,
n.minobsinnode = c(20, 40))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
## cross validation on boosted tree method to find optima tuning parameters
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ##
## creates grid of possible tuning parameters
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7),
n.trees = c(1:20)*10 ,
shrinkage = 0.1,
n.minobsinnode = c(20, 40))
## sets trainControl method
fit_control <- trainControl(method = "repeatedcv",
number = 5,
repeats= 1)
set.seed(13)
## trains to find optimal tuning parameters except it is giving weird parameters
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
method = "gbm",
preProcess = c("center", "scale"),
trControl = fit_control,
tuneGrid= gbm_grid,
verbose=FALSE)
## plot to visualize parameters
plot(gbm_tree_cv)
cor(entertainment_train)[1,] %>% sort()
## delete## exploritory to try to figure out which variables to include in models cause weird results##
cor(entertainment_train)[1,] %>% sort()
cor(lifestyle_train)[1,] %>% sort()
=======
train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)
# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
#Shares table for lifestyle_train
tables(lifestyle_train)
#Correlation table for lifestyle_train
correlation.table(lifestyle_train)
#Correlation graph for lifestyle_train
corr_simple(lifestyle_train)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
#Dropped url and timedelta because they are non-predictive.
online <- online[ , c(3:61)]
#All trained data sets are in another .Rmd file called Create_dataframes_use_later. These can be copy and pasted when they are necessary
#Creates Summary Tables for a training dataset
tables <- function(data_input) {
d <- describe(lifestyle_train[ , c('shares')], fast=TRUE)
kable(d, caption = 'Shares Summary')
}
#Create correlation table and graph for a training dataset
correlation.table <- function(data_input) {
#drop binary variables
correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
kable(correlations, caption = 'Correlations Lifestyle')
}
# Create correlation graph
corr_simple <- function(data_input,sig=0.5){
corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
corr <- subset(corr, abs(value) > 0.5)
corr[order(-abs(corr$value)),]
print(corr)
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
# Still working this one out
# Create scatterplots
# scatterplots <- function(data_input, xwert, y = shares) {
#   g <- ggplot(data_input, aes(x = xwert, y = shares))
#   g + geom_point(size = 2)
# }
# scatterplots(data_input = lifestyle_train, x = n_unique_tokens)
# ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()
# # Change the point size, and shape
# ggplot(mtcars, aes(x=wt, y=mpg)) +
#   geom_point(size=2, shape=23)
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)]
#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
filter(data_channel_is_lifestyle == 1)
#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)]
lifestyle <- lifestyle[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)
# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
#Shares table for lifestyle_train
tables(lifestyle_train)
#Correlation table for lifestyle_train
correlation.table(lifestyle_train)
#Correlation graph for lifestyle_train
corr_simple(lifestyle_train)
scatterplots <- function(data_input) {
ggplot(data_input, aes(x = kw_min_min, y = shares)) +
geom_point()
}
scatterplots(lifestyle_train)
scatterplots <- function(data_input, x) {
ggplot(data = data_input, aes(x = x, y = shares)) +
geom_point()
}
scatterplots(lifestyle_train, kw_min_min)
scatterplots <- function(data_input, x) {
data_input %>%
ggplot(aes(x = x, y = shares)) + geom_point()
}
# Still working this one out
# Create scatterplots
scatterplots <- function(data_input, x) {
data_input %>%
ggplot(aes(x = x, y = shares)) + geom_point()
}
scatterplots(lifestyle_train, kw_min_min)
scatterplots <- function(data_input, xval) {
data_input %>%
ggplot(aes(x = xval, y = shares)) + geom_point()
}
scatterplots(lifestyle_train, kw_min_min)
scatterplots <- function(data_input) {
data_input %>%
ggplot(aes(x = kw_min_min, y = shares)) + geom_point()
}
scatterplots(lifestyle_train)
install.packages('cowplot')
library(cowplot)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("~/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2")
setwd("~/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents")
setwd("~/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
setwd('C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2')
online <- read.csv('OnlineNewsPopularity.csv')
setwd('C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2/')
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
setwd("~/GitHub/ST_558_Project_2")
setwd("C:/Documents/Github")
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
setwd("C:/Documents/Github")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
setwd("C:/Documents/Github/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
setwd("C:/Documents/Github/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
colnames(online) <- c('url', 'days', 'n.Title', 'n.Content', 'Rate.Unique',
'Rate.Nonstop', 'Rate.Unique.Nonstop', 'n.Links',
'n.Other', 'n.Images', 'n.Videos',
'Avg.Words', 'n.Key', 'Lifestyle', 'Entertainment',
'Business', 'Social.Media', 'Tech', 'World', 'Min.Worst.Key',
'Max.Worst.Key', 'Avg.Worst.Key', 'Min.Best.Key',
'Max.Best.Key', 'Avg.Best.Key', 'Avg.Min.Key', 'Avg.Max.Key',
'Avg.Avg.Key', 'Min.Ref', 'Max.Ref', 'Avg.Ref', 'Mon',
'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun', 'Weekend',
'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04',
'Global.Subj', 'Global.Pol', 'Global.Pos.Rate',
'Global.Neg.Rate', 'Rate.Pos', 'Rate.Neg', 'Avg.Pos.Pol',
'Min.Pos.Pol', 'Max.Pos.Pol', 'Avg.Neg.Pol', 'Min.Neg.Pol',
'Max.Neg.Pol', 'Title.Subj', 'Title.Pol', 'Abs.Subj',
'Abs.Pol', 'shares')
#Dropped url and timedelta because they are non-predictive.
online <- online[ , c(3:61)]
#All trained data sets are in another .Rmd file called Create_dataframes_use_later. These can be copy and pasted when they are necessary
## I'm not 100% sure but I think we are supposed to run this same process with the different params set up. so its like just one doc that we knit with the six different params and it goes through and does everything without us needing to copy and paste
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)]
#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
filter(Lifestyle == 1)
#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)]
lifestyle <- lifestyle[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)
# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
fit_control <- trainControl(method='repeatedcv', number=3, repeats=1)
forest <- train(shares ~.,
data = lifestyle_train,
method ='rf',
ntree = 200,
trControl = fit_control)
set.seed(123)
fit_control <- trainControl(method='repeatedcv', number=3, repeats=1)
forest <- train(shares ~.,
data = lifestyle_train,
method ='rf',
ntree = 200,
importance = TRUE,
trControl = fit_control)
rfFit <- randomForest(price ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
forest$importance
forest$finalModel$importance
e <- as.data.frame(forest$finalModel$importance)
(varImpPlot(e$%IncMSE))
e <- e %>% select(c('%IncMSE'))
(varImpPlot(e))
(varImpPlot(forest))
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
attributes(rfFit)
attributes(forest)
as.tibble(rfFit$importance)
View(e)
varImpPlot(rfFit)
varImpPlot(rfFit,scale=TRUE,n.var=18)
varImpPlot(rfFit,scale=TRUE,n.var=10)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.tibble(rfFit$importance)
varImpPlot(rfFit,scale=TRUE,n.var=10)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE,n.var=10)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE,n.var=10)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
set.seed(10210526)
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Cross validation not needed on random forest
set.seed(10210526)
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/3,
ntree = 200, importance = TRUE)
as.data.frame(rfFit$importance)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
e <- as.data.frame(rfFit$importance)
e <- e %>%
select(c('%IncMSE'))
max(e)
View(e)
rfFit
#Cross validation not needed on random forest
set.seed(10210526)
rfFit <- randomForest(shares ~ ., data = lifestyle_train,
mtry = ncol(lifestyle_train)/5,
ntree = 200, importance = TRUE)
rfFit
e <- as.data.frame(rfFit$importance)
e
e <- e %>%
select(c('%IncMSE'))
max(e)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Principal Component Analysis
dimensions <- dim(lifestyle_train)
n_rows <- dimensions[1]
n_columns <- dimensions[2]
cat("Number of rows:", n_rows, "\n")
cat("Number of columns:", n_columns, "\n")
colSums(is.na(lifestyle_train))
install.packages("corrr")
library(corrr)
data_normalized <- scale(lifestyle_train)
corr_matrix <- cor(data_normalized)
corr_matrix
install.packages('ggcorrplot')
library(ggcorrplot)
ggcorrplot(corr_matrix)
install.packages("FactoMineR")
install.packages("factoextra")
library(FactoMineR)
library(factoextra)
data.pca <- princomp(corr_matrix)
summary(data.pca)
cor(lifestyle_train)[1,] %>% sort()
#Removed rate.nonstop because they were all 1. Removed Mon-Sat and Weekend because they were binary/categorical
lifestyle_train2 <- lifestyle_train %>%
select(-c('Rate.Nonstop',
'Mon', 'Tues',
'Wed', 'Thurs',
'Fri', 'Sat',
'Sun',
'Weekend'))
#Cross validation not needed on random forest
set.seed(10210526)
#Removed rate.nonstop because they were all 1. Removed Mon-Sat and Weekend because they were binary/categorical
lifestyle_train2 <- lifestyle_train %>%
select(-c('Rate.Nonstop',
'Mon', 'Tues',
'Wed', 'Thurs',
'Fri', 'Sat',
'Sun',
'Weekend'))
rfFit <- randomForest(shares ~ ., data = lifestyle_train2,
mtry = ncol(lifestyle_train2)/3,
ntree = 200, importance = TRUE)
rfFit
e <- as.data.frame(rfFit$importance)
e
e <- e %>%
select(c('%IncMSE'))
max(e)
varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Please look over this. What do you think? There are far too many variables, but I am not sure which ones to get rid of that I haven't already.
>>>>>>> 77f97f48eab0c5dd7ee97ba3f7b1e6cbc5a470b8
