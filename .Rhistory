Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
Weekday <- mutate(Weekday,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
## creates a contingency table of shareshigh and whether it is a weekday
table(shareshigh$shareshigh, Weekday$Weekday)
## creates  a contingency table of shareshigh and whether it is monday
table(Weekday$Days_of_Week, shareshigh$shareshigh)
## creates  a contingency table of shareshigh and whether it is monday
c <- table(Weekday$Days_of_Week, shareshigh$shareshigh)
prop.table(c, margin=1)
addmargins(prop.table(c, margin=1))
prop.table(c, margin=1)
## creates a contingency table of shareshigh and whether it is a weekday
prop.table(table(shareshigh$shareshigh, Weekday$Weekday))
## creates  a contingency table of shareshigh and whether it is monday
prop.table(table(Weekday$Days_of_Week, shareshigh$shareshigh))
Weekday$Days_of_Week <- ordered(Weekday$Days_of_Week,
levels=c("Mon", "Tues",
"Wedn", "Thurs",
"Fri", "Weekend"))
## creates  a contingency table of shareshigh and whether it is monday
prop.table(table(Weekday$Days_of_Week, shareshigh$shareshigh))
## create a new column that combines Mon-Fri into weekdays
Weekday <- mutate(data_channel_train,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
Weekday <- mutate(Weekday,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
Weekday$Days_of_Week <- ordered(Weekday$Days_of_Week,
levels=c("Mon", "Tues",
"Wed", "Thurs",
"Fri", "Weekend"))
## creates  a contingency table of shareshigh and whether it is monday
prop.table(table(Weekday$Days_of_Week, shareshigh$shareshigh))
## create a new column that combines Mon-Fri into weekdays
Weekday <- mutate(data_channel_train,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
Weekday <- mutate(Weekday,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
## creates  a contingency table of shareshigh and whether it is monday
prop.table(table(Weekday$Days_of_Week, shareshigh$shareshigh))
## mean of shares
mean(data_channel_train$shares)
## sd of shares
sd(data_channel_train$shares)
## creates a new column that is if shares is higher than average or not
shareshigh <- data_channel_train %>% select(shares) %>% mutate (shareshigh = (shares> mean(shares)))
## create a new column that combines Mon-Fri into weekdays
Weekday <- mutate(data_channel_train,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
Weekday <- mutate(Weekday,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
Weekday$Days_of_Week <- ordered(Weekday$Days_of_Week,
levels=c("Mon", "Tues",
"Wed", "Thurs",
"Fri", "Weekend"))
## creates a contingency table of shareshigh and whether it is a weekday
kable(prop.table(table(shareshigh$shareshigh, Weekday$Weekday)))
## creates  a contingency table of shareshigh and whether it is monday
kable(prop.table(table(Weekday$Days_of_Week, shareshigh$shareshigh)))
## mean of shares
mean(data_channel_train$shares)
## sd of shares
sd(data_channel_train$shares)
## creates a new column that is if shares is higher than average or not
shareshigh <- data_channel_train %>% select(shares) %>% mutate (shareshigh = (shares> mean(shares)))
## create a new column that combines Mon-Fri into weekdays
shareshigh <- mutate(shareshigh,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
## mean of shares
mean(data_channel_train$shares)
## sd of shares
sd(data_channel_train$shares)
## creates a new column that is if shares is higher than average or not
shareshigh <- data_channel_train %>% mutate (shareshigh = (shares> mean(shares)))
## create a new column that combines Mon-Fri into weekdays
shareshigh <- mutate(shareshigh,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
shareshigh <- mutate(shareshigh,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
shareshigh$Days_of_Week <- ordered(shareshigh$Days_of_Week,
levels=c("Mon", "Tues",
"Wed", "Thurs",
"Fri", "Weekend"))
## creates a contingency table of shareshigh and whether it is a weekday
kable(prop.table(table(shareshigh$shareshigh,
shareshigh$Weekday)))
## creates  a contingency table of shareshigh and whether it is monday
kable(prop.table(table(shareshigh$Days_of_Week, shareshigh$shareshigh)))
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
geom_bar
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
geom_bar()
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
geom_bar(aes(y = (..count..)/sum(..count..)))
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
geom_bar(aes(y = (after_stat(count))/sum(after_stat(count))))
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
geom_bar(aes(y = (..count..)/sum(..count..)))
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Weekday or Weekend?')
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Weekday or Weekend?') +
ylab('Relative Frequency')
ggplot(shareshigh, aes(x = Days_of_Week, fill = shareshigh)) +
geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Day of the Week') +
ylab('Relative Frequency')
ggplot(shareshigh, aes(x = shareshigh, fill = Days_of_Week)) +
geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Day of the Week') +
ylab('Relative Frequency')
ggplot(shareshigh, aes(x = Days_of_Week, fill = shareshigh)) +
geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Day of the Week') +
ylab('Relative Frequency')
## mean of shares
mean(data_channel_train$shares)
## sd of shares
sd(data_channel_train$shares)
## creates a new column that is if shares is higher than average or not
shareshigh <- data_channel_train %>% mutate (shareshigh = (shares> mean(shares)))
## create a new column that combines Mon-Fri into weekdays
shareshigh <- mutate(shareshigh,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
shareshigh <- mutate(shareshigh,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
shareshigh$Days_of_Week <- ordered(shareshigh$Days_of_Week,
levels=c("Mon", "Tues",
"Wed", "Thurs",
"Fri", "Weekend"))
## creates a contingency table of shareshigh and whether it is a weekday
print(prop.table(table(shareshigh$shareshigh,
shareshigh$Weekday)))
## creates  a contingency table of shareshigh and the day of the week
print(prop.table(table(shareshigh$Days_of_Week,
shareshigh$shareshigh)))
## creates a new column that is if shares is higher than average or not
shareshigh <- data_channel_train %>% mutate (shareshigh = (shares> mean(shares)))
## create a new column that combines Mon-Fri into weekdays
shareshigh <- mutate(shareshigh,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
shareshigh <- mutate(shareshigh,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
shareshigh$Days_of_Week <- ordered(shareshigh$Days_of_Week,
levels=c("Mon", "Tues",
"Wed", "Thurs",
"Fri", "Weekend"))
## creates a contingency table of shareshigh and whether it is a weekday
print(prop.table(table(shareshigh$shareshigh,
shareshigh$Weekday)))
## creates  a contingency table of shareshigh and the day of the week
print(prop.table(table(shareshigh$Days_of_Week,
shareshigh$shareshigh)))
## creates a new column that is if shares is higher than average or not
shareshigh <- data_channel_train %>% mutate (shareshigh = (shares> mean(shares)))
## create a new column that combines Mon-Fri into weekdays
shareshigh <- mutate(shareshigh,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
shareshigh <- mutate(shareshigh,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
shareshigh$Days_of_Week <- ordered(shareshigh$Days_of_Week,
levels=c("Mon", "Tues",
"Wed", "Thurs",
"Fri", "Weekend"))
## creates a contingency table of shareshigh and whether it is a weekday
print(prop.table(table(shareshigh$Weekday,
shareshigh$shareshigh)))
## creates  a contingency table of shareshigh and the day of the week
print(prop.table(table(shareshigh$Days_of_Week,
shareshigh$shareshigh)))
ggplot(shareshigh, aes(x=Rate.Pos, y=Rat.Neg,
color=Days_of_Week)) +
geom_point(size=6)
ggplot(shareshigh, aes(x=Rate.Pos, y=Rate.Neg,
color=Days_of_Week)) +
geom_point(size=6)
ggplot(shareshigh, aes(x=n.Content, y=Avg.Avg.Key,
color=Days_of_Week)) +
geom_point(size=6)
ggplot(shareshigh, aes(x=n.Content, y=Avg.Avg.Key,
color=Days_of_Week)) +
geom_point(size=3)
ggplot(shareshigh, aes(x=n.Content, y=Avg.Avg.Key,
color=Weekday)) +
geom_point(size=3)
ggplot(shareshigh, aes(x=Rate.Pos, y=Rate.Neg,
color=Days_of_Week)) +
geom_point(size=2)
ggplot(data_channel_train, aes(x=n.Links)) +
geom_histogram(color="black", fill="white")
ggplot(data_channel_train, aes(x=n.Links, y = shares)) +
geom_histogram(color="black", fill="white")
ggplot(shareshigh, aes(x=n.Links, y=shares,
color=Days_of_Week)) +
geom_point(size=2)
## DELETE## This will also only knit htmls, we have to change the output to github_document and use the one at the bottom to get it to generate github documents
## creates a list of all 6 desired params from online
data_channel_is <- c("Lifestyle", "Entertainment", "Business", "Social.Media", "Tech", "World")
output_file <- paste0(data_channel_is, ".html")
#create a list for each team with just the team name parameter
params = lapply(data_channel_is, FUN = function(x){list(DataChannel = x)})
#put into a data frame
reports <- tibble(output_file, params)
## renders with params to all
apply(reports, MARGIN=1, FUN = function(x){
# rmarkdown::render("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2/_Rmd/ST_558_Project_2.Rmd", output_dir = "./automations_test2_html", output_file = x[[1]], params = x[[2]]
rmarkdown::render('C:/Documents/Github/ST_558_Project_2/_Rmd/ST_558_Project_2.Rmd', output_dir = "./automations_test2_html", output_file = x[[1]], params = x[[2]]
)
}
)
set.seed(10210526)
rfFit <- train(shares ~ .,
data = data_channel_train,
method = "rf",
trControl = trainControl(method = "cv",
number = 5),
preProcess = c("center", "scale"),
mtry = ncol(data_channel_train)/3)
rfFit <- train(shares ~ .,
data = data_channel_train,
method = "rf",
trControl = trainControl(method = "cv",
number = 5),
preProcess = c("center", "scale"),
tuneGrid =
data.frame(mtry = ncol(data_channel_train)/3))
set.seed(10210526)
rfFit <- train(shares ~ .,
data = data_channel_train,
method = "rf",
n.trees = 100,
trControl = trainControl(method = "cv",
number = 5),
preProcess = c("center", "scale"),
tuneGrid =
data.frame(mtry = ncol(data_channel_train)/3))
## DELETE## changed this to eval = FALSE so it would knit on my machine please feel free to change back if i forget
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
setwd("C:/Documents/Github/ST_558_Project_2")
#setwd("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
#Shares table for data_channel_train
summary_table(data_channel_train)
#Correlation table for lifestyle_train
correlation_table(data_channel_train)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
library(data.table)
#Correlation table for lifestyle_train
correlation_table(data_channel_train)
## creates a new column that is if shares is higher than average or not
shareshigh <- data_channel_train %>% mutate (shareshigh = (shares> mean(shares)))
## create a new column that combines Mon-Fri into weekdays
shareshigh <- mutate(shareshigh,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
shareshigh <- mutate(shareshigh,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
shareshigh$Days_of_Week <- ordered(shareshigh$Days_of_Week,
levels=c("Mon", "Tues",
"Wed", "Thurs",
"Fri", "Weekend"))
## creates a contingency table of shareshigh and whether it is a weekday
print(prop.table(table(shareshigh$Weekday,
shareshigh$shareshigh)))
#Correlation graph for lifestyle_train
correlation_graph(data_channel_train)
View(data_channel_test)
## creates a new column that is if shares is higher than average or not
shareshigh <- data_channel_train %>% mutate (shareshigh = (shares> mean(shares)))
## create a new column that combines Mon-Fri into weekdays
shareshigh <- mutate(shareshigh,
Weekday = ifelse(Mon == 1 |
Tues ==1 |
Wed == 1 |
Thurs == 1 |
Fri == 1,
'Weekday', 'Weekend'))
shareshigh <- mutate(shareshigh,
Days_of_Week = ifelse(Mon == 1 &
Weekday == 'Weekday', 'Mon',
ifelse(Tues == 1  &
Weekday == "Weekday", 'Tues',
ifelse(Wed == 1 &
Weekday == "Weekday", 'Wed',
ifelse(Thurs ==1 &
Weekday == 'Weekday', 'Thurs',
ifelse(Fri == 1 &
Weekday == 'Weekday',
'Fri', 'Weekend'))))))
shareshigh$Days_of_Week <- ordered(shareshigh$Days_of_Week,
levels=c("Mon", "Tues",
"Wed", "Thurs",
"Fri", "Weekend"))
## creates a contingency table of shareshigh and whether it is a weekday
print(prop.table(table(shareshigh$Weekday,
shareshigh$shareshigh)))
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Weekday or Weekend?') +
ylab('Relative Frequency')
ggplot(shareshigh, aes(x=Rate.Pos, y=Rate.Neg,
color=Days_of_Week)) +
geom_point(size=2)
## creates  a contingency table of shareshigh and the day of the week
print(prop.table(table(shareshigh$Days_of_Week,
shareshigh$shareshigh)))
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Weekday or Weekend?') +
ylab('Relative Frequency')
ggplot(shareshigh, aes(x = Days_of_Week, fill = shareshigh)) +
geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Day of the Week') +
ylab('Relative Frequency')
set.seed(10210526)
rfFit <- train(shares ~ .,
data = data_channel_train,
method = "rf",
n.trees = 100,
trControl = trainControl(method = "cv",
number = 5),
preProcess = c("center", "scale"),
tuneGrid =
data.frame(mtry = 1:sqrt(ncol(data_channel_train))))
set.seed(10210526)
rfFit <- train(shares ~ .,
data = data_channel_train,
method = "rf",
trControl = trainControl(method = "cv",
number = 5),
preProcess = c("center", "scale"),
tuneGrid =
data.frame(mtry = 1:sqrt(ncol(data_channel_train))))
rfFit_pred <- predict(rfFit, newdata = data_channel_test)
rfRMSE<- postResample(rfFit_pred, obs =
data_channel_test$shares)
##Delete## I will try again to add a tuneGrid to the random forest tomorrow later today
## Delete## i think it looks good! i will need to rewatch the lecture and try to dig in some more cause im really lost on these ensamble tree based models. i think you chose some good ones to remove.  the only thing is in the project it says "Both models should be chosen using cross-validation" for random forest and boosted tree, so i think the random forest does need cv unless im missing something. Im still trying to figure out what is wrong with my cross validation and a way to take the tuning parameters based on each data_channel_is during the automation process. I think we are close though!
## Delete## I rewatched the lecture for model fitting using the caret package. I made the appropriate modifications. I tried to use the tuneGrid but the code was trying to run for over 5 minutes and it never went through, so I used the suggested mtry for regression from the random forest lecture.
## delete## So i went through the random forrest stuff again while doing the hw that was due today and my random forrest also took forever to run with the tune grid. i think it just depends on the size of the expand.grid that you give it. we should keep that in mind for when we are doing the final render of the project because it may take forever to render all 6 lol
## DELETE## This will also only knit htmls, we have to change the output to github_document and use the one at the bottom to get it to generate github documents
## creates a list of all 6 desired params from online
data_channel_is <- c("Lifestyle", "Entertainment", "Business", "Social.Media", "Tech", "World")
output_file <- paste0(data_channel_is, ".html")
#create a list for each team with just the team name parameter
params = lapply(data_channel_is, FUN = function(x){list(DataChannel = x)})
#put into a data frame
reports <- tibble(output_file, params)
## renders with params to all
apply(reports, MARGIN=1, FUN = function(x){
# rmarkdown::render("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2/_Rmd/ST_558_Project_2.Rmd", output_dir = "./automations_test2_html", output_file = x[[1]], params = x[[2]]
rmarkdown::render('C:/Documents/Github/ST_558_Project_2/_Rmd/ST_558_Project_2.Rmd', output_dir = "./automations_test2_html", output_file = x[[1]], params = x[[2]]
)
}
)
##Delete## So, I changed all necessary inputs to data_channel_train and data_channel_test. I can change the default YAML and see that it does create do everything for the data channel specified in the YAML. That's awesome! But, it is running all 6 data channels at once? If that is the case, where are the htmls going for each data channel? I see the folder with htmls in it, but the htmls have a bunch of 0s and 1s.
## DELETe## so i think the issue may be you have to run the actual code chuck and then change the "c:/Users towhatever the path of this file is on your device and then the output_dir to whereever you want the 6 documents to render ( you can see all 6 rendered from running this code chunk in the automations_test folder ). but it wont work if you knit it the normal way. we will also need to make sure it is giuthub_document for the final so it shows up properly
##About the written commentary, are we supposed to provide that for every data_channel? If so, how are we automating that? We still need to add the factor contingency graphs. Do we want to set a binary to shares like we did for the HW8 for rented bike count? If so, what is your suggestion? I do not mind making the tables for that. As far as the blog post goes, I do not mind writing it. I do have trouble with github in the following ways 1.) Even though I use and .md file, getting my output images to show up in my blog posts, 2.) I do not know how to set a visually appealing theme.
## DELETE## i dont think so. so we just need to write general commentary where the project says "As you will automate this same analysis across other data, you can’t describe the trends you see in the graph (unless you want to try to automate that!)" which i honestly dont want to try to automate that lol and i dont think we get any points for that so unless you really want to automate i would just write general commentary. as far as the contigency tables and graphs i think the project says we are both supposed to make some contigency tables and three graphs, "Each group member is responsible for producing some summary statistics (means, sds, contingency tables, etc.) and for producing at least three graphs (each) of the data."  so for my contigency tables (make sure these are two way tables i asked him and he emphatically said two way tables lol) i just did shares like a factor of above or below the mean shares (called it shareshigh) and then compared it to the weekend and monday and then made a histogram and col graph and scatterplot of some differnt variables. so i would just do something along those lines with whatever variables you think are interesting.  i think we also both have to write out own blog post but like i will use the links to your repo and pages, we dont have to put the actual docs in the blog ( they will be in the repo and the links will be in the readme) . as far as the output i think that may be because the output has to be github_document? im not sure on that one ive never had that issue but i can look into it. i wouldnt worry about a visually appealing theme unless its something you care about, i didnt pick a theme at all for my pages for project 1 and didnt lose any points for it. i think unless its in the grading rubric or notes on grading on the project 2 document then we wont get dinged for it. sorry i wrote so much! i proabably wont be on later today after i finish my commentary but if you have any issues feel free to text me at 7192898738 or email me at dlsamara@ncsu.edu and i will get on (or tomorrow morning depending on the time) and we can make sure everything is working for the submission tomorrow.
