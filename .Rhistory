kable(correlations, caption = 'Correlations Lifestyle')
}
correlation.table(lifestyle_train)
corr <- cor(lifestyle_train)
corr <- corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- cor(lifestyle_train)
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
View(corr)
corr <- subset(corr, abs(value) > 0.5)
View(corr)
corr[order(-abs(corr$value)),]
corr_simple <- function(data_input,sig=0.5){
corr <- cor(data_input)
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
corr <- subset(corr, abs(value) > 0.5)
corr[order(-abs(corr$value)),]
print(corr)
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
corr_simple(lifestyle_train)
# Create correlation graph
corr_simple <- function(data_input,sig=0.5){
corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
corr <- subset(corr, abs(value) > 0.5)
corr[order(-abs(corr$value)),]
print(corr)
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
corr_simple(lifestyle_train)
View(lifestyle_train)
library(ggplot2)
scatterplots <- function(data_input, x, y) {
g <- ggplot(data_input, aes(x = x, y = y))
g + geom_point(size = 2)
}
scatterplots <- function(data_input, x) {
g <- ggplot(data_input, aes(x = x, y = shares))
g + geom_point(size = 2)
}
scatterplots(lifestyle_train, n_unique_tokens)
scatterplots(data_input = lifestyle_train, x = n_unique_tokens)
scatterplots <- function(data_input, x) {
g <- ggplot(data_input, aes(x, y = shares))
g + geom_point(size = 2)
}
scatterplots(data_input = lifestyle_train, x = n_unique_tokens)
scatterplots <- function(data_input, xwert, y = shares) {
g <- ggplot(data_input, aes(x = xwert, y = shares))
g + geom_point(size = 2)
}
scatterplots(data_input = lifestyle_train, x = n_unique_tokens)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
#Dropped url and timedelta because they are non-predictive.
online <- online[ , c(3:61)]
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
entertainment <- online[ , -c(12, 14:17)]
#Filter out the zeros from the remaining data_channel_is column
entertainment <- entertainment %>%
filter(data_channel_is_entertainment == 1)
#Drop the data_channel_is column
entertainment <- entertainment[ , -c(12)]
entertainment <- entertainment[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(entertainment), size = nrow(entertainment)*.70)
test <- setdiff(1:nrow(entertainment), train)
# trainiing and testing subsets
entertainment_train <- entertainment[train, ]
entertainment_test <- entertainment[test, ]
#Create data_channel_is_bus dataset
#Take out all data_Channel_is columns that are not relevant to this data set
business <- online[ , -c(12:13, 15:17)]
#Filter out the zeros from the remaining data_channel_is column
business <- business %>%
filter(data_channel_is_bus == 1)
#Drop the data_channel_is column
business <- business[ , -c(12)]
business <- business[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(business), size = nrow(business)*.70)
test <- setdiff(1:nrow(business), train)
# trainiing and testing subsets
business_train <- business[train, ]
business_test <- business[test, ]
#Create data_channel_is_socmed dataset
#Take out all data_Channel_is columns that are not relevant to this data set
social_media <- online[ , -c(12:14, 16:17)]
#Filter out the zeros from the remaining data_channel_is column
social_media <- social_media %>%
filter(data_channel_is_socmed == 1)
#Drop the data_channel_is column
social_media <- social_media[ , -c(12)]
social_media <- social_media[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(social_media), size = nrow(social_media)*.70)
test <- setdiff(1:nrow(social_media), train)
# trainiing and testing subsets
social_media_train <- social_media[train, ]
social_media_test <- social_media[test, ]
#Create data_channel_is_tech dataset
#Take out all data_Channel_is columns that are not relevant to this data set
tech <- online[ , -c(12:15, 17)]
#Filter out the zeros from the remaining data_channel_is column
tech <- tech %>%
filter(data_channel_is_tech == 1)
#Drop the data_channel_is column
tech <- tech[ , -c(12)]
tech <- tech[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(tech), size = nrow(tech)*.70)
test <- setdiff(1:nrow(tech), train)
# trainiing and testing subsets
tech_train <- tech[train, ]
tech_test <- tech[test, ]
#Create data_channel_is_world dataset
#Take out all data_Channel_is columns that are not relevant to this data set
world <- online[ , -c(12:16)]
#Filter out the zeros from the remaining data_channel_is column
world <- world %>%
filter(data_channel_is_world == 1)
#Drop the data_channel_is column
world <- world[ , -c(12)]
world <- world[ , c(53, 1:52)]
#Split the data into training and test#
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(world), size = nrow(world)*.70)
test <- setdiff(1:nrow(world), train)
# trainiing and testing subsets
world_train <- world[train, ]
world_test <- world[test, ]
#Creates Summary Tables for a training dataset
tables <- function(data_input) {
d <- describe(lifestyle_train[ , c('shares')], fast=TRUE)
kable(d, caption = 'Shares Summary')
}
#Create correlation table and graph for a training dataset
correlation.table <- function(data_input) {
#drop binary variables
correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
kable(correlations, caption = 'Correlations Lifestyle')
}
correlation.table(lifestyle_train)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
#Dropped url and timedelta because they are non-predictive.
online <- online[ , c(3:61)]
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
entertainment <- online[ , -c(12, 14:17)]
#Filter out the zeros from the remaining data_channel_is column
entertainment <- entertainment %>%
filter(data_channel_is_entertainment == 1)
#Drop the data_channel_is column
entertainment <- entertainment[ , -c(12)]
entertainment <- entertainment[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(entertainment), size = nrow(entertainment)*.70)
test <- setdiff(1:nrow(entertainment), train)
# trainiing and testing subsets
entertainment_train <- entertainment[train, ]
entertainment_test <- entertainment[test, ]
#Create data_channel_is_bus dataset
#Take out all data_Channel_is columns that are not relevant to this data set
business <- online[ , -c(12:13, 15:17)]
#Filter out the zeros from the remaining data_channel_is column
business <- business %>%
filter(data_channel_is_bus == 1)
#Drop the data_channel_is column
business <- business[ , -c(12)]
business <- business[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(business), size = nrow(business)*.70)
test <- setdiff(1:nrow(business), train)
# trainiing and testing subsets
business_train <- business[train, ]
business_test <- business[test, ]
#Create data_channel_is_socmed dataset
#Take out all data_Channel_is columns that are not relevant to this data set
social_media <- online[ , -c(12:14, 16:17)]
#Filter out the zeros from the remaining data_channel_is column
social_media <- social_media %>%
filter(data_channel_is_socmed == 1)
#Drop the data_channel_is column
social_media <- social_media[ , -c(12)]
social_media <- social_media[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(social_media), size = nrow(social_media)*.70)
test <- setdiff(1:nrow(social_media), train)
# trainiing and testing subsets
social_media_train <- social_media[train, ]
social_media_test <- social_media[test, ]
#Create data_channel_is_tech dataset
#Take out all data_Channel_is columns that are not relevant to this data set
tech <- online[ , -c(12:15, 17)]
#Filter out the zeros from the remaining data_channel_is column
tech <- tech %>%
filter(data_channel_is_tech == 1)
#Drop the data_channel_is column
tech <- tech[ , -c(12)]
tech <- tech[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(tech), size = nrow(tech)*.70)
test <- setdiff(1:nrow(tech), train)
# trainiing and testing subsets
tech_train <- tech[train, ]
tech_test <- tech[test, ]
#Create data_channel_is_world dataset
#Take out all data_Channel_is columns that are not relevant to this data set
world <- online[ , -c(12:16)]
#Filter out the zeros from the remaining data_channel_is column
world <- world %>%
filter(data_channel_is_world == 1)
#Drop the data_channel_is column
world <- world[ , -c(12)]
world <- world[ , c(53, 1:52)]
#Split the data into training and test#
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(world), size = nrow(world)*.70)
test <- setdiff(1:nrow(world), train)
# trainiing and testing subsets
world_train <- world[train, ]
world_test <- world[test, ]
#Creates Summary Tables for a training dataset
tables <- function(data_input) {
d <- describe(lifestyle_train[ , c('shares')], fast=TRUE)
kable(d, caption = 'Shares Summary')
}
#Create correlation table and graph for a training dataset
correlation.table <- function(data_input) {
#drop binary variables
correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
kable(correlations, caption = 'Correlations Lifestyle')
}
# Create correlation graph
corr_simple <- function(data_input,sig=0.5){
corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
corr <- subset(corr, abs(value) > 0.5)
corr[order(-abs(corr$value)),]
print(corr)
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
# #Create scatterplots
# scatterplots <- function(data_input, xwert, y = shares) {
#   g <- ggplot(data_input, aes(x = xwert, y = shares))
#   g + geom_point(size = 2)
# }
# scatterplots(data_input = lifestyle_train, x = n_unique_tokens)
# ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()
# # Change the point size, and shape
# ggplot(mtcars, aes(x=wt, y=mpg)) +
#   geom_point(size=2, shape=23)
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)]
#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
filter(data_channel_is_lifestyle == 1)
#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)]
lifestyle <- lifestyle[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)
# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
#Shares table for lifestyle_train
tables(lifestyle_train)
#Correlation table for lifestyle_train
correlation.table(lifestyle_train)
#Correlation graph for lifestyle_train
corr_simple(lifestyle_train)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
#Dropped url and timedelta because they are non-predictive.
online <- online[ , c(3:61)]
#All trained data sets are in another .Rmd file called Create_dataframes_use_later. These can be copy and pasted when they are necessary
#Creates Summary Tables for a training dataset
tables <- function(data_input) {
d <- describe(lifestyle_train[ , c('shares')], fast=TRUE)
kable(d, caption = 'Shares Summary')
}
#Create correlation table and graph for a training dataset
correlation.table <- function(data_input) {
#drop binary variables
correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
kable(correlations, caption = 'Correlations Lifestyle')
}
# Create correlation graph
corr_simple <- function(data_input,sig=0.5){
corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
corr[lower.tri(corr, diag = TRUE)] <- NA
corr <- melt(corr, na.rm = TRUE)
corr <- subset(corr, abs(value) > 0.5)
corr[order(-abs(corr$value)),]
print(corr)
mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
# Still working this one out
# Create scatterplots
# scatterplots <- function(data_input, xwert, y = shares) {
#   g <- ggplot(data_input, aes(x = xwert, y = shares))
#   g + geom_point(size = 2)
# }
# scatterplots(data_input = lifestyle_train, x = n_unique_tokens)
# ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()
# # Change the point size, and shape
# ggplot(mtcars, aes(x=wt, y=mpg)) +
#   geom_point(size=2, shape=23)
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)]
#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
filter(data_channel_is_lifestyle == 1)
#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)]
lifestyle <- lifestyle[ , c(53, 1:52)]
#Split the data into training and test
set.seed(5432)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)
# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
#Shares table for lifestyle_train
tables(lifestyle_train)
#Correlation table for lifestyle_train
correlation.table(lifestyle_train)
#Correlation graph for lifestyle_train
corr_simple(lifestyle_train)
scatterplots <- function(data_input) {
ggplot(data_input, aes(x = kw_min_min, y = shares)) +
geom_point()
}
scatterplots(lifestyle_train)
scatterplots <- function(data_input, x) {
ggplot(data = data_input, aes(x = x, y = shares)) +
geom_point()
}
scatterplots(lifestyle_train, kw_min_min)
scatterplots <- function(data_input, x) {
data_input %>%
ggplot(aes(x = x, y = shares)) + geom_point()
}
# Still working this one out
# Create scatterplots
scatterplots <- function(data_input, x) {
data_input %>%
ggplot(aes(x = x, y = shares)) + geom_point()
}
scatterplots(lifestyle_train, kw_min_min)
scatterplots <- function(data_input, xval) {
data_input %>%
ggplot(aes(x = xval, y = shares)) + geom_point()
}
scatterplots(lifestyle_train, kw_min_min)
scatterplots <- function(data_input) {
data_input %>%
ggplot(aes(x = kw_min_min, y = shares)) + geom_point()
}
scatterplots(lifestyle_train)
install.packages('cowplot')
library(cowplot)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("~/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2")
setwd("~/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents")
setwd("~/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
setwd('C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2')
online <- read.csv('OnlineNewsPopularity.csv')
setwd('C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2/')
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
online <- read.csv('OnlineNewsPopularity.csv')
setwd("C:/Users/kmgolden/Documents/GitHub/ST_558_Project_2/")
online <- read.csv('OnlineNewsPopularity.csv')
