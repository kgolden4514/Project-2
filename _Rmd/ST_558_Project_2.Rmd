---
title: "Project 2"
author: "Kristina Golden and Demetrios Samaras"
date: "2023-07-02"
output: github_document
params: 
  DataChannel: "Lifestyle"
---

```{r render with params, eval=FALSE, include=FALSE}
## creates a list of all 6 desired params from online
data_channel_is <- c("Lifestyle", "Entertainment", "Business", "Social.Media", "Tech", "World")

output_file <- paste0(data_channel_is, ".html")
#create a list for each team with just the team name parameter
params = lapply(data_channel_is, FUN = function(x){list(DataChannel = x)})
#put into a data frame
reports <- tibble(output_file, params)

## renders with params to all 
apply(reports, MARGIN=1, FUN = function(x){
  
# rmarkdown::render("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2/_Rmd/ST_558_Project_2.Rmd", output_dir = "./automations_test2_html", output_file = x[[1]], params = x[[2]]
                  
rmarkdown::render('C:/Documents/Github/ST_558_Project_2/_Rmd/ST_558_Project_2.Rmd', output_dir = "./automations_test2_html", output_file = x[[1]], params = x[[2]]
    )
  }
)
```

```{r setup , include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
setwd("C:/Documents/Github/ST_558_Project_2")
#setwd("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
```

# `r params$DataChannel`

## Required Packages

```{r packages, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
library(data.table)
```

## Introduction

In this report we will be looking at the `r params$DataChannel` data channel of the online news popularity data set. This data set looks at a wide range of variables from 39644 different news articles. The response variable that we will be focusing on is **shares**. The purpose of this analysis is to try to predict how many shares a `r params$DataChannel` article will get based on the values of those other variables. We will be modeling shares using two different linear regression models and two ensemble tree based models.

## Read in the Data

```{r data, echo=TRUE, message=FALSE, warning=FALSE}



setwd("C:/Documents/Github/ST_558_Project_2")
#setwd("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2")
 

online <- read.csv('OnlineNewsPopularity.csv')
colnames(online) <- c('url', 'days', 'n.Title', 'n.Content', 'Rate.Unique', 
                      'Rate.Nonstop', 'Rate.Unique.Nonstop', 'n.Links', 
                      'n.Other', 'n.Images', 'n.Videos',
                      'Avg.Words', 'n.Key', 'Lifestyle', 'Entertainment',
                      'Business', 'Social.Media', 'Tech', 'World', 'Min.Worst.Key',
                      'Max.Worst.Key', 'Avg.Worst.Key', 'Min.Best.Key', 
                      'Max.Best.Key', 'Avg.Best.Key', 'Avg.Min.Key', 'Avg.Max.Key',
                      'Avg.Avg.Key', 'Min.Ref', 'Max.Ref', 'Avg.Ref', 'Mon', 
                      'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun', 'Weekend',
                      'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 
                      'Global.Subj', 'Global.Pol', 'Global.Pos.Rate',
                      'Global.Neg.Rate', 'Rate.Pos', 'Rate.Neg', 'Avg.Pos.Pol',
                      'Min.Pos.Pol', 'Max.Pos.Pol', 'Avg.Neg.Pol', 'Min.Neg.Pol',
                      'Max.Neg.Pol', 'Title.Subj', 'Title.Pol', 'Abs.Subj',
                      'Abs.Pol', 'shares')
#Dropped url and timedelta because they are non-predictive. 
online <- online[ , c(3:61)]


```

## Write Functions

```{r summary_table}
summary_table <- function(data_input) {
    min <- min(data_input$shares)
    q1 <- quantile(data_input$shares, 0.25)
    med <- median(data_input$shares)
    q3 <- quantile(data_input$shares, 0.75)
    max <- max(data_input$shares)
    mean1 <- mean(data_input$shares)
    sd1 <- sd(data_input$shares)
    data <- matrix(c(min, q1, med, q3, max, mean1, sd1), 
                   ncol=1)
    rownames(data) <- c("Minimum", "Q1", "Median", "Q3",
                           "Maximum", "Mean", "SD")
    colnames(data) <- c('Shares')
    data <- as.table(data)
    data
}
```

```{r correlation_table}
#Create correlation table and graph for a training dataset
correlation_table <- function(data_input) {
  #drop binary variables
  correlations <- cor(subset(data_input, select = c(2:4, 6:24,
                                                    33:50)))
  kable(correlations, caption = 'Correlations Lifestyle')
}

```

```{r correlation_graph}
# Create correlation graph
correlation_graph <- function(data_input,sig=0.5){
  corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
  corr[lower.tri(corr, diag = TRUE)] <- NA
  corr <- melt(corr, na.rm = TRUE)
  corr <- subset(corr, abs(value) > 0.5)
  corr[order(-abs(corr$value)),]
  print(corr)
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
```

## `r params$DataChannel` EDA

### `r params$DataChannel`

```{r automation creation of each data set, message=FALSE }

## filters rows based on when parameter is 1 
data_channel <-  online %>% filter( !!rlang::sym(params$DataChannel) == 1)

## Drop the data_channel_is columns 
data_channel <- data_channel[ , -c(12:17)]

## reorder to put shares first 
data_channel <- data_channel[ , c(53, 1:52)]

```

```{r automation train and test }

set.seed(5432)

# Split the data into a training and test set (70/30 split)
# indices

train <- sample(1:nrow(data_channel), size = nrow(data_channel)*.70)
test <- setdiff(1:nrow(data_channel), train)

# training and testing subsets
data_channel_train <- data_channel[train, ]
data_channel_test <- data_channel[test, ]


```

## `r params$DataChannel` Summarizations

```{r r params$DataChannel summary_table}
#Shares table for data_channel_train
summary_table(data_channel_train)
   
```

The above table displays the `r params$DataChannel` 5-number summary for the shares. It also includes the mean and standard deviation. Because the mean is greater than the median, we suspect that the `r params$DataChannel` shares distribution is right skewed.

```{r r params$DataChannel correlation_table}
#Correlation table for lifestyle_train
correlation_table(data_channel_train)
```

The above table gives the correlations between all variables in the `r params$DataChannel` data set. This allows us to see which two variables have strong correlation. If we have two variables with a high correlation, we might want to remove one of them to avoid too much multicollinearity.

```{r r params$DataChannel corr_graph, message=FALSE, warning=FALSE}
#Correlation graph for lifestyle_train
correlation_graph(data_channel_train)
```

Because the correlation table above is large, it can be difficult to read. The correlation graph above gives a visual summary of the table. Using the legend, we are able to see the correlations between variables, how strong the correlation is, and in what direction.

```{r scatterplot}
ggplot(shareshigh, aes(x=Rate.Pos, y=Rate.Neg,
                       color=Days_of_Week)) +
    geom_point(size=2)
```

Once seeing the correlation table and graph, it is possible to graph two variables on a scatterplot. This provides a visual of the linear relationship. A scatterplot of two variables in the `r params$DataChannel` dataset has been created above.

```{r contingency tables }
## mean of shares 
mean(data_channel_train$shares)

## sd of shares 
sd(data_channel_train$shares)

## creates a new column that is if shares is higher than average or not 
shareshigh <- data_channel_train %>% select(shares) %>% mutate (shareshigh = (shares> mean(shares)))

## creates a contingency table of shareshigh and whether it is the weekend 
table(shareshigh$shareshigh, data_channel_train$Weekend)

```

These above contingency tables will look at the shareshigh factor which says whether the number of shares is higher than the mean number of shares or not and compares it to the weekend. Using these we can see if the number of shares tends to be higher or not on the weekend.

```{r weekday tables}
## creates a new column that is if shares is higher than average or not 
shareshigh <- data_channel_train %>% mutate (shareshigh = (shares> mean(shares)))

## create a new column that combines Mon-Fri into weekdays
shareshigh <- mutate(shareshigh, 
                  Weekday = ifelse(Mon == 1 |
                                     Tues ==1 |
                                     Wed == 1 |
                                     Thurs == 1 |
                                     Fri == 1, 
                                    'Weekday', 'Weekend'))
shareshigh <- mutate(shareshigh, 
                  Days_of_Week = ifelse(Mon == 1 & 
                                Weekday == 'Weekday', 'Mon',
                              ifelse(Tues == 1  &
                                Weekday == "Weekday", 'Tues',
                              ifelse(Wed == 1 &
                                Weekday == "Weekday", 'Wed',
                              ifelse(Thurs ==1 &
                                Weekday == 'Weekday', 'Thurs',
                              ifelse(Fri == 1 & 
                                       Weekday == 'Weekday',
                                     'Fri', 'Weekend'))))))

shareshigh$Days_of_Week <- ordered(shareshigh$Days_of_Week, 
                                levels=c("Mon", "Tues",
                                         "Wed", "Thurs", 
                                         "Fri", "Weekend"))

## creates a contingency table of shareshigh and whether it is a weekday 
print(prop.table(table(shareshigh$Weekday,
                       shareshigh$shareshigh)))
```

The contingency table above looks at the before-mentioned shareshigh factor and compares it to the whether the day was a weekend or a weekday. This allows us to see if shares tend to be higher on weekends or weekdays. The frequencies are displayed as relative frequencies.

```{r days of week}
## creates  a contingency table of shareshigh and the day of the week
a <- prop.table(table(shareshigh$Days_of_Week,
                 shareshigh$shareshigh))
b <- as.data.frame(a)
print(a)
```

After comparing shareshigh with whether or not the day was a weekend or weekday, the above contingency table compares shareshigh for each specific day of the week. Again, the frequencies are displayed as relative frequencies.

```{r weekday bar graph}
ggplot(shareshigh, aes(x = Weekday, fill = shareshigh)) +
  geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Weekday or Weekend?') + 
  ylab('Relative Frequency')
```

```{r day of the week graph}
ggplot(shareshigh, aes(x = Days_of_Week, fill = shareshigh)) +
  geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) + xlab('Day of the Week') + 
  ylab('Relative Frequency')
```

The above bar graphs are a visual representation of the contingency tables between weekends/weekdays and shareshigh and the days of the week and shareshigh.. Graphs can improve the stakeholders ability to interpret the results quickly.

```{r most frequent}

a <- table(shareshigh$Days_of_Week)
# a <- prop.table(table(shareshigh$Days_of_Week,
#                  shareshigh$shareshigh))
b <- as.data.frame(a)
colnames(b) <- c('Day of Week', 'Freq')
b <- filter(b, Freq == max(b$Freq))
d <- as.character(b[1,1])
g <- mutate(shareshigh, 
                  Most_Freq = ifelse(Days_of_Week == d,
                                    'Most Freq Day',
                                    'Not Most Freq Day'
                                    ))
paste0(" For ", 
        params$DataChannel, " ", 
       d, " is the most frequent day of the week")

table(shareshigh$shareshigh, g$Most_Freq)
```

The above contingency table compares shareshigh to the `r params$DataChannel` day that occurs most frequently. This allows us to see if the most frequent day tends to have more shareshigh.

```{r shares histogram }
## creates plotting object of shares
a <- ggplot(data_channel_train, aes(x=shares))

## histogram of shares 
a+geom_histogram(color= "red", fill="blue")+ ggtitle("Shares histogram")

```

Above we can see the frequency distribution of shares of the `r params$DataChannel` data channel. We should always see a long tail to the right because a small number of articles will get a very high number of shares. But looking at by looking at the distribution we can say how many shares most of these articles got.

```{r col graph }
## creates plotting object with number of words in title and shares
b<- ggplot(data_channel_train, aes(x=n.Title, y=shares))

## creates a bar chart with number of words in title and shares 
b+ geom_col(fill="blue")+ ggtitle("Number of words in title vs shares") + labs(x="Number of words in title")
```

In the above graph we are looking at the number of shares based on how many words are in the title of the article. if we see a large peak on at the higher number of words it means for this data channel there were more shares on longer titles, and if we see a peak at smaller number of words then there were more shares on smaller titles.

```{r correlations between shares and other variables, eval=TRUE }
## makes correlation of every variable with shares 
shares_correlations <- cor(data_channel_train)[1,] %>% sort() 

shares_correlations
```

```{r graph of shares with highest correlated var}
## take the name of the highest correlated variable
highest_cor <-shares_correlations[52]  %>% names()

highest_cor

## creats scatter plot looking at shares vs highest correlated variable
g <-ggplot(data_channel_train,  aes(y=shares, x= data_channel_train[[highest_cor]])) 


g+ geom_point(aes(color=as.factor(Weekend))) +geom_smooth(method = lm) + ggtitle(" Highest correlated variable with shares") + labs(x="Highest correlated variable vs shares", color="Weekend")
```

The above graph looks at the relationship between shares and the variable with the highest correlation for the `r params$DataChannel` data channel, and colored based on whether or not it is the weekend. because this is the most positively correlated variable we should always see an upward trend but the more correlated they are the more the dots will fall onto the line of best fit.

## Modeling

## Linear Regression

Linear regression is a tool with many applications available to data scientists. In linear regression, a linear relationship between one dependent variable and one or more independent variables is assumed. In computerized linear regression, many linear regressions between the response variable and the explanatory variable(s) are calculated. The regression that is considered the "best fit" is the least squares regression line. To determine the LSRL, the sum of the squared residuals is calculated for each regression. The best model is the regression that minimizes the sum of the squared residuals. Linear regression is used to predict responses for explanatory variable(s); it is also used to examine trends in the data.

### Linear regression 1

```{r Linear regression 1, message=FALSE, warning=FALSE}
## linear regression model using all predictors 
set.seed(13)

linear_model_1 <- train( shares ~ ., 
                         data = data_channel_train,
                         method = "lm",
                         preProcess = c("center", "scale"),
                         trControl = trainControl(method = "cv", 
                                                  number = 5))

## prediction of test with model 
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(data_channel_test, -shares))

## storing error of model on test set 
linear_1_RMSE<- postResample(linear_model_1_pred, obs = data_channel_test$shares)

```

### Linear regression 2

```{r Linear regression 2, message=FALSE, warning=FALSE}

#Removed rate.Nonstop because it was only 1 and removed the days of the week.
linear_model_2 <- train( shares ~. - Rate.Nonstop - Mon
                         - Tues - Wed - Thurs - Fri - Sat
                         - Sun - Weekend, 
                        data = data_channel_train,
                         method = "lm",
                         preProcess = c("center", 
                                        "scale"),
                         trControl = trainControl(
                           method= "cv", 
                           number = 5))
## prediction of test with model 
linear_model_2_pred <- predict(linear_model_2, newdata = dplyr::select(data_channel_test, -shares))

## storing error of model on test set 
linear_2_RMSE<- postResample(linear_model_2_pred, obs = data_channel_test$shares)

```

## Ensemble Models

### Random forest model

A random forest model is used in machine learning to generate predictions or classifications. This is done through generating many decision trees on many different samples and taking the average (regression) or the majority vote (classification). Some of the benefits to using random forest models are that over-fitting is minimized and the model works with the presence of categorical and continuous variables. With increased computer power and the increased knowledge in machine learning, random forest models will continue to grow in popularity.

```{r random_forest }
set.seed(10210526)
rfFit <- train(shares ~ ., 
        data = data_channel_train,
        method = "rf",
        trControl = trainControl(method = "cv",
                                        number = 5),
        preProcess = c("center", "scale"),
        tuneGrid = 
          data.frame(mtry = 1:sqrt(ncol(data_channel_train))))
rfFit_pred <- predict(rfFit, newdata = data_channel_test)
rfRMSE<- postResample(rfFit_pred, obs =
                            data_channel_test$shares)
```

### Boosted tree model

A decision tree makes a binary decision based on the value input. A boosted tree model generates a predictive model based on an ensemble of decision trees where better trees are generated based on the performance of previous trees. Our boosted tree model can be tuned using four different parameters: interaction.depth which defines the complexity of the trees being built, n.trees which defines the number of trees built (number of iterations), shrinkage which dictates the rate at which the algorithm learns, and n.minobsinnode which dictates the number of samples left to allow for a node to split.

```{r boosted tree tuning }
 

## creates grid of possible tuning parameters 
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7), 
  n.trees = c(1:20) , 
  shrinkage = 0.1,
  n.minobsinnode = c(10,20, 40))

## sets trainControl method 
fit_control <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats= 1)

set.seed(13)

## trains to find optimal tuning parameters except it is giving weird parameters 
gbm_tree_cv <- train(shares ~ . , data = data_channel_train,
                     method = "gbm",
                     preProcess = c("center", "scale"),
                     trControl = fit_control,
                     tuneGrid= gbm_grid,
                     verbose=FALSE)
## plot to visualize parameters 
plot(gbm_tree_cv)

## test set prediction
boosted_tree_model_pred <- predict(gbm_tree_cv, newdata = dplyr::select(data_channel_test, -shares), n.trees = 7)

## stores results 
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = data_channel_test$shares)

```

## Comparison

```{r comparison of 4 models }
## creates a data frame of the four models RMSE on the 
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
                         linear_2_RMSE=linear_2_RMSE[1], 
                         rfRMSE=rfRMSE[1],
                          boosted_tree_RMSE =
                           boosted_tree_RMSE[1] )

models_RMSE

## gets the name of the column with the smallest rmse 
smallest_RMSE<-colnames(models_RMSE)[apply(models_RMSE,1,which.min)]

## declares the model with smallest RSME the winner 
paste0(" For ", 
        params$DataChannel, " ", 
       smallest_RMSE, " is the winner")

```

## Automation

This is the code used to automate the rendering of each document based on the parameter of data_channel_is designated in the YAML.

```{r render with params code, echo=TRUE, eval=FALSE}

## creates a list of all 6 desired params from online
data_channel_is <- c("Lifestyle", "Entertainment", "Business", "Social.Media", "Tech", "World")

## creates the output file name 
output_file <- paste0(data_channel_is, ".md")

#create a list for each channel with just the channel name parameter
params = lapply(data_channel_is, FUN = function(x){list(DataChannel = x)})

#put into a data frame
reports <- tibble(output_file, params)

## renders with params to all based on rows in reports
apply(reports, MARGIN=1, FUN = function(x){
## change first path to wherever yours is and output_dir to whatever folder you want it to output to   
rmarkdown::render('C:/Documents/Github/ST_558_Project_2/_Rmd/ST_558_Project_2.Rmd', output_dir = "./automations_test2_md", output_file = x[[1]], params = x[[2]]
    )
  }
)

```
