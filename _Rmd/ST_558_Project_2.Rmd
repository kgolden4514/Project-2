---
title: "Project 2"
author: "Kristina Golden and Demetrios Samaras"
date: "2023-07-02"
output: html_document
params: 
  DataChannel: "Lifestyle"
---

```{r render with params, echo=FALSE, eval=FALSE, include=FALSE}
## delete ## got the automation to work, onve we have everything else good to go i will go through and adapt the code for it. we just need to use  params$DataChannel. see the automation_test for more details ## 

## creates a list of all 6 desired params from online
data_channel_is <- c("Lifestyle", "Entertainment", "Business", "Social.Media", "Tech", "World")

output_file <- paste0(data_channel_is, ".html")
#create a list for each team with just the team name parameter
params = lapply(data_channel_is, FUN = function(x){list(DataChannel = x)})
#put into a data frame
reports <- tibble(output_file, params)

## renders with params to all 
apply(reports, MARGIN=1, FUN = function(x){
  
rmarkdown::render('C:/Documents/Github/ST_558_Project_2/_Rmd/Automation_test.Rmd', output_dir = "./automations_test_html", output_file = x[[1]], params = x[[2]]
    )
  }
)

##Delete## Ok, besides written commentary, I think everything else is good to go (unless we decide we want to add some scatterplots, but there are so many possible prediction variables, that scatterplots seem overwhelming). What do I need to do to assist with the automation portion? I am happy to assist, though I do find this portion extremely confusing. I really appreciate you taking that part of the project on! Let me know if there is anything you would like me to modify on my portions of the regression/EDA

## Delete## Yep I think the last thing we have to do is the written commentary and then the summarzations portion where we each have to generate summary stats like means contingency tables and three graphs each. i think the automation portion should be pretty straight forward from here since you already did everything to sort the data we basically just have to use the object from this 
`data_channel <-  online %>% filter( !!rlang::sym(params$DataChannel)== 1)`
# which fills data_channel with all the rows that have a 1 for that parameter.  so rather than using lifestyle_train etc it will be data_channel_train and then each report will generate from each data channel but it will go through and do all the graphs and stuff with the data from that. the default in the yaml is lifestyle so if you want to look at a differnt data set while we are still working on this document you can change it to like "Tech" to make sure the code works but its finicky so check it with params$DataChannel, I have to knit to get it to update the params$DataChannel . but when we run the render it will generate all 6. we also need to change it to github document and .md for the final product. 

# If you are still confused about the automation please let me know and i will try to a better job explaining what is going on. it definitely took me a bit to figure out how it works.  

```

```{r setup, eval= FALSE, include=FALSE}

## DELETE## changed this to eval = FALSE so it would knit on my machine please feel free to change back if i forget
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, 
                      cache = TRUE)
setwd("C:/Documents/Github/ST_558_Project_2")
options(scipen = 1, digits = 6)
```

## Required Packages

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
```

## Introduction

In this report we will be looking at the `r params$DataChannel` data channel of the online news popularity data set. This data set looks at a wide range of variables from 39644 different news articles. The response variable that we will be focusing on is **shares**. The purpose of this analysis is to try to predict how many shares a `r params$DataChannel` article will get based on the values of those other variables. We will be modeling shares using two different linear regression models and two ensemble tree based models. 


## Read in the Data

```{r data, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE}
## DELETE## changed this so it would knit on my machine feel free to change it back if I forget to

#setwd("C:/Documents/Github/ST_558_Project_2")
setwd("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
colnames(online) <- c('url', 'days', 'n.Title', 'n.Content', 'Rate.Unique', 
                      'Rate.Nonstop', 'Rate.Unique.Nonstop', 'n.Links', 
                      'n.Other', 'n.Images', 'n.Videos',
                      'Avg.Words', 'n.Key', 'Lifestyle', 'Entertainment',
                      'Business', 'Social.Media', 'Tech', 'World', 'Min.Worst.Key',
                      'Max.Worst.Key', 'Avg.Worst.Key', 'Min.Best.Key', 
                      'Max.Best.Key', 'Avg.Best.Key', 'Avg.Min.Key', 'Avg.Max.Key',
                      'Avg.Avg.Key', 'Min.Ref', 'Max.Ref', 'Avg.Ref', 'Mon', 
                      'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun', 'Weekend',
                      'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 
                      'Global.Subj', 'Global.Pol', 'Global.Pos.Rate',
                      'Global.Neg.Rate', 'Rate.Pos', 'Rate.Neg', 'Avg.Pos.Pol',
                      'Min.Pos.Pol', 'Max.Pos.Pol', 'Avg.Neg.Pol', 'Min.Neg.Pol',
                      'Max.Neg.Pol', 'Title.Subj', 'Title.Pol', 'Abs.Subj',
                      'Abs.Pol', 'shares')
#Dropped url and timedelta because they are non-predictive. 
online <- online[ , c(3:61)]

#All trained data sets are in another .Rmd file called Create_dataframes_use_later. These can be copy and pasted when they are necessary

## I'm not 100% sure but I think we are supposed to run this same process with the different params set up. so its like just one doc that we knit with the six different params and it goes through and does everything without us needing to copy and paste 
```

## Write Functions

```{r summary_table}
#Creates Summary Tables for a training dataset
summary_table <- function(data_input) {
  d <- describe(data_input[ , c('shares')], fast=TRUE)
  kable(d, caption = 'Shares Summary')
}
```

```{r correlation_table}
#Create correlation table and graph for a training dataset
correlation_table <- function(data_input) {
  #drop binary variables
  correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
  kable(correlations, caption = 'Correlations Lifestyle')
}
```

```{r correlation_graph}
# Create correlation graph
correlation_graph <- function(data_input,sig=0.5){
  corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
  corr[lower.tri(corr, diag = TRUE)] <- NA
  corr <- melt(corr, na.rm = TRUE)
  corr <- subset(corr, abs(value) > 0.5)
  corr[order(-abs(corr$value)),]
  print(corr)
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
```

```{r scatterplots_WIP}
# Still working this one out
# Create scatterplots
# scatterplots <- function(data_input) {
#   
# }
# scatterplots(lifestyle_train)
```



## `r params$DataChannel` 

### `r params$DataChannel` EDA

```{r automation creation of each data set, message=FALSE }

## filters rows based on when parameter is 1 
data_channel <-  online %>% filter( !!rlang::sym(params$DataChannel) == 1)

## Drop the data_channel_is columns 
data_channel <- data_channel[ , -c(12:17)]

## reorder to put shares first 
data_channel <- data_channel[ , c(53, 1:52)]

```

```{r automation train and test }

set.seed(5432)

# Split the data into a training and test set (70/30 split)
# indices

train <- sample(1:nrow(data_channel), size = nrow(data_channel)*.70)
test <- setdiff(1:nrow(data_channel), train)

# training and testing subsets
data_channel_train <- data_channel[train, ]
data_channel_test <- data_channel[test, ]


```



```{r Lifestyle}
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)] 


#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
             filter(Lifestyle == 1)

#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)] 
lifestyle <- lifestyle[ , c(53, 1:52)]
```

```{r lifestyle_train_test}
#Split the data into training and test
set.seed(5432)

# Split the data into a training and test set (70/30 split)
# indices

train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)

# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
```

## `r params$DataChannel` Summarizations 

```{r lifestyle_summary}
#Shares table for lifestyle_train
summary_table(lifestyle_train)
```

```{r lifestyle_corr_table}
#Correlation table for lifestyle_train
correlation_table(lifestyle_train)
```

```{r lifestyle_corr_graph}
#Correlation graph for lifestyle_train
correlation_graph(lifestyle_train)
```



```{r correlations between shares and other variables, eval=TRUE }
## makes least to most of the correlation of every variable with shares 
shares_correlations <- cor(data_channel_train)[1,] %>% sort() 

shares_correlations

```

```{r graph of shares with highest correlated var}
## take the name of the highest correlated variable
highest_cor <-shares_correlations[52]  %>% names()

## creats scatter plot looking at shares vs highest correlated variable
g <-ggplot(data_channel_train,  aes(y=shares, x= data_channel_train[[highest_cor]])) 

g+ geom_point() + labs + ggtitle(" Highest correlated variable with shares")

```

```{r cpntogency table }
#at least 2 factors 
#shares exceeded median and wether program was on the weekend 
#wether or not it was the weekend 
# cut() 
#online %>% select(shares) %>% mutate (shareshigh> mean shares)

#is weekend shares high %>% table prop table relative freq

# prop.table(margin= "isweekend")
```





## Modeling

### Linear regression 1

```{r Linear regression 1 }
## linear regression model using all predictors 
set.seed(13)

linear_model_1 <- train( shares ~ ., 
                         data = data_channel_train,
                         method = "lm",
                         preProcess = c("center", "scale"),
                         trControl = trainControl(method = "cv", 
                                                  number = 5))

## prediction of test with model 
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(data_channel_test, -shares))

## storing error of model on test set 
linear_1_RMSE<- postResample(linear_model_1_pred, obs = data_channel_test$shares)

```

### Linear regression 2

```{r Linear regression 2 }
## Delete ## i think this looks good! smart things to cut out, i will try to improve mine. ## 

#Removed rate.Nonstop because it was only 1 and removed the days of the week.
linear_model_2 <- train( shares ~. - Rate.Nonstop - Mon
                         - Tues - Wed - Thurs - Fri - Sat
                         - Sun - Weekend, 
                        data = lifestyle_train,
                         method = "lm",
                         preProcess = c("center", 
                                        "scale"),
                         trControl = trainControl(
                           method= "cv", 
                           number = 5))
## prediction of test with model 
linear_model_2_pred <- predict(linear_model_2, newdata = dplyr::select(lifestyle_test, -shares))

## storing error of model on test set 
linear_2_RMSE<- postResample(linear_model_2_pred, obs = lifestyle_test$shares)

```

### Random forest model

```{r random_forest, cache=TRUE}
set.seed(10210526)
rfFit <- train(shares ~ ., 
        data = lifestyle_train,
        method = "rf",
        trControl = trainControl(method = "cv",
                                        number = 5),
        preProcess = c("center", "scale"),
        tuneGrid = 
          data.frame(mtry = ncol(lifestyle_train)/3))
rfFit_pred <- predict(rfFit, newdata = lifestyle_test)
rfRMSE<- postResample(rfFit_pred, obs =
                            lifestyle_test$shares)

## Delete## i think it looks good! i will need to rewatch the lecture and try to dig in some more cause im really lost on these ensamble tree based models. i think you chose some good ones to remove.  the only thing is in the project it says "Both models should be chosen using cross-validation" for random forest and boosted tree, so i think the random forest does need cv unless im missing something. Im still trying to figure out what is wrong with my cross validation and a way to take the tuning parameters based on each data_channel_is during the automation process. I think we are close though! 

## Delete## I rewatched the lecture for model fitting using the caret package. I made the appropriate modifications. I tried to use the tuneGrid but the code was trying to run for over 5 minutes and it never went through, so I used the suggested mtry for regression from the random forest lecture.

## delete## So i went through the random forrest stuff again while doing the hw that was due today and my random forrest also took forever to run with the tune grid. i think it just depends on the size of the expand.grid that you give it. we should keep that in mind for when we are doing the final render of the project because it may take forever to render all 6 lol

```

### Boosted tree model

```{r boosted tree tuning, cache=TRUE}

## cross validation on boosted tree method to find optima tuning parameters 
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ## 

## creates grid of possible tuning parameters 
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7), 
  n.trees = c(1:20) , 
  shrinkage = 0.1,
  n.minobsinnode = c(10,20, 40))

## sets trainControl method 
fit_control <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats= 1)

set.seed(13)

## trains to find optimal tuning parameters except it is giving weird parameters 
gbm_tree_cv <- train(shares ~ . , data = data_channel_train,
                     method = "gbm",
                     preProcess = c("center", "scale"),
                     trControl = fit_control,
                     tuneGrid= gbm_grid,
                     verbose=FALSE)
## plot to visualize parameters 
plot(gbm_tree_cv)

## test set prediction
boosted_tree_model_pred <- predict(gbm_tree_cv, newdata = dplyr::select(data_channel_test, -shares), n.trees = 7)

## stores results 
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = data_channel_test$shares)

```


## Comparison

```{r comparison of 4 models }
## creates a data frame of the four models RMSE on the 
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
                         linear_2_RMSE=linear_2_RMSE[1], 
                         rfRMSE=rfRMSE[1],
                          boosted_tree_RMSE =
                           boosted_tree_RMSE[1] )

## gets the name of the column with the smallest rmse 
smallest_RMSE<-colnames(models_RMSE)[apply(models_RMSE,1,which.min)]

## declares the model with smallest RSME the winner 
paste0(" For ", 
        params$DataChannel, " ", 
       smallest_RMSE, " is the winner")

```

## Automation 

This is the code used to automate the rendering of each document based on the parameter designated in the YAML. 

```{r render with params code, echo=TRUE, eval=FALSE}

## creates a list of all 6 desired params from online
data_channel_is <- c("Lifestyle", "Entertainment", "Business", "Social.Media", "Tech", "World")

## creates the output file name 
output_file <- paste0(data_channel_is, ".md")

#create a list for each channel with just the channel name parameter
params = lapply(data_channel_is, FUN = function(x){list(DataChannel = x)})

#put into a data frame
reports <- tibble(output_file, params)

## renders with params to all 
apply(reports, MARGIN=1, FUN = function(x){
  
rmarkdown::render('C:/Documents/Github/ST_558_Project_2/_Rmd/Automation_test.Rmd', output_dir = "./automations_test_html", output_file = x[[1]], params = x[[2]]
    )
  }
)

```


