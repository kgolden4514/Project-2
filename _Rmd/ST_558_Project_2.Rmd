---
title: "Project 2"
author: "Kristina Golden and Demetrios Samaras"
date: "2023-07-02"
output: html_document
params: 
      data_channel_is: 
---

```{r render with params, echo=FALSE, eval=FALSE, include=FALSE}
## delete ## working on the automation, let me know if you think there is a better way. i think for this we just have to go through at put params$ ## 

## renders single github doc with parameters lifestyle 
rmarkdown::render("/GitHub/ST_558_Project_2", output_file = "Lifestyle.md",
params = list(data_channel_is = "Lifestyle"))

## renders all 6 documents 

## creates a list of all 6 desired params from online
X <- c("Lifestyle", "Entertainment", "Business", "Social.Media", "Tech", "World")

## renders with params to all 
apply(X, FUN = function(X){
  
out <- paste0(X, ".md")
  
rmarkdown::render("/GitHub/ST_558_Project_2", output_file = out,
params = list(data_channel_is = X ))
  
  
})


```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
setwd("C:/Documents/Github/ST_558_Project_2")
options(scipen = 1, digits = 6)
```

## Required Packages

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
```

## Introduction

**this is just a placeholder for now, will do better when I have a better understanding of what is going on**

## Read in the Data

```{r data, echo=FALSE, message=FALSE, warning=FALSE}
setwd("C:/Documents/Github/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
colnames(online) <- c('url', 'days', 'n.Title', 'n.Content', 'Rate.Unique', 
                      'Rate.Nonstop', 'Rate.Unique.Nonstop', 'n.Links', 
                      'n.Other', 'n.Images', 'n.Videos',
                      'Avg.Words', 'n.Key', 'Lifestyle', 'Entertainment',
                      'Business', 'Social.Media', 'Tech', 'World', 'Min.Worst.Key',
                      'Max.Worst.Key', 'Avg.Worst.Key', 'Min.Best.Key', 
                      'Max.Best.Key', 'Avg.Best.Key', 'Avg.Min.Key', 'Avg.Max.Key',
                      'Avg.Avg.Key', 'Min.Ref', 'Max.Ref', 'Avg.Ref', 'Mon', 
                      'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun', 'Weekend',
                      'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 
                      'Global.Subj', 'Global.Pol', 'Global.Pos.Rate',
                      'Global.Neg.Rate', 'Rate.Pos', 'Rate.Neg', 'Avg.Pos.Pol',
                      'Min.Pos.Pol', 'Max.Pos.Pol', 'Avg.Neg.Pol', 'Min.Neg.Pol',
                      'Max.Neg.Pol', 'Title.Subj', 'Title.Pol', 'Abs.Subj',
                      'Abs.Pol', 'shares')
#Dropped url and timedelta because they are non-predictive. 
online <- online[ , c(3:61)]

#All trained data sets are in another .Rmd file called Create_dataframes_use_later. These can be copy and pasted when they are necessary

## I'm not 100% sure but I think we are supposed to run this same process with the different params set up. so its like just one doc that we knit with the six different params and it goes through and does everything without us needing to copy and paste 
```

# Write Functions

```{r summary_table}
#Creates Summary Tables for a training dataset
summary_table <- function(data_input) {
  d <- describe(data_input[ , c('shares')], fast=TRUE)
  kable(d, caption = 'Shares Summary')
}
```

```{r correlation_table}
#Create correlation table and graph for a training dataset
correlation_table <- function(data_input) {
  #drop binary variables
  correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
  kable(correlations, caption = 'Correlations Lifestyle')
}
```

```{r correlation_graph}
# Create correlation graph
correlation_graph <- function(data_input,sig=0.5){
  corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
  corr[lower.tri(corr, diag = TRUE)] <- NA
  corr <- melt(corr, na.rm = TRUE)
  corr <- subset(corr, abs(value) > 0.5)
  corr[order(-abs(corr$value)),]
  print(corr)
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
```

```{r scatterplots_WIP}
# Still working this one out
# Create scatterplots
# scatterplots <- function(data_input) {
#   
# }
# scatterplots(lifestyle_train)
```

```{r scaterplots testing, include=FALSE, echo=FALSE, eval=FALSE}
## delete ## just looking at the way scatter plots look 

g <-ggplot(lifestyle_train,  aes(y=shares, x= num_imgs))

g+ geom_point()

```

# Lifestyle

## Lifestyle EDA

```{r Lifestyle}
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)] 


#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
             filter(Lifestyle == 1)

#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)] 
lifestyle <- lifestyle[ , c(53, 1:52)]
```

```{r lifestyle_train_test}
#Split the data into training and test
set.seed(5432)

# Split the data into a training and test set (70/30 split)
# indices

train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)

# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
```

```{r lifestyle_summary}
#Shares table for lifestyle_train
summary_table(lifestyle_train)
```

```{r lifestyle_corr_table}
#Correlation table for lifestyle_train
correlation_table(lifestyle_train)
```

```{r lifestyle_corr_graph}
#Correlation graph for lifestyle_train
correlation_graph(lifestyle_train)
```

```{r correlations between shares and other variables }
## delete## exploratory to try to figure out which variables to include in models cause weird results. might need to prescreen and pick less variables for modeling  ## 

cor(entertainment_train)[1,] %>% sort()


cor(lifestyle_train)[1,] %>% sort()


```

# Modeling

## Linear regression 1

```{r Linear regression 1 }
## linear regression model using all predictors 
linear_model_1 <- train( shares ~ ., data = entertainment_train,
                         method = "lm",
                         preProcess = c("center", "scale"),
                         trControl = trainControl(method = "cv", 
                                                  number = 5))

## prediction of test with model 
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(entertainment_test, -shares))

## storing error of model on test set 
linear_1_RMSE<- postResample(linear_model_1_pred, obs = entertainment_test$shares)

```

## Linear regression 2

```{r Linear regression 2 }
## delete ## I think we can just copy and paste linear regression 1 with slightly different predictors after the ~ ?  let me know what you think. ##

```

## Random forest model

```{r random_forest}
#Cross validation not needed on random forest
set.seed(10210526)
#Removed rate.nonstop because they were all 1. Removed Mon-Sat and Weekend because they were binary/categorical
lifestyle_train2 <- lifestyle_train %>%
                      select(-c('Rate.Nonstop',
                                'Mon', 'Tues',
                                'Wed', 'Thurs',
                                'Fri', 'Sat',
                                'Sun', 
                                'Weekend'))
rfFit <- randomForest(shares ~ ., data = lifestyle_train2, 
                      mtry = ncol(lifestyle_train2)/3,
                      ntree = 200, importance = TRUE)
rfFit
e <- as.data.frame(rfFit$importance)
e
e <- e %>%
      select(c('%IncMSE'))
max(e)

varImpPlot(rfFit,scale=TRUE, var = 20)
rfPred <- predict(rfFit, newdata =
                    dplyr::select(lifestyle_test, -shares))
rfRMSE <- sqrt(mean((rfPred-lifestyle_test$shares)^2))
rfRMSE
#Please look over this. What do you think? There are far too many variables, but I am not sure which ones to get rid of that I haven't already.

```

## Boosted tree model

```{r boosted tree tuning, cache=TRUE}

## cross validation on boosted tree method to find optima tuning parameters 
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ## 

## creates grid of possible tuning parameters 
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7), 
  n.trees = c(1:20)*10 , 
  shrinkage = 0.1,
  n.minobsinnode = c(20, 40))

## sets trainControl method 
fit_control <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats= 1)

set.seed(13)

## trains to find optimal tuning parameters except it is giving weird parameters 
gbm_tree_cv <- train(shares ~ ., data = entertainment_train,
                     method = "gbm",
                     preProcess = c("center", "scale"),
                     trControl = fit_control,
                     tuneGrid= gbm_grid,
                     verbose=FALSE)
## plot to visualize parameters 
plot(gbm_tree_cv)

```

```{r boosted tree tuned}
## creates boosted tree model with tuning parameters n.trees, shrinkage, interaction.depth from CV 
boosted_tree_model <- gbm(shares ~ ., data = entertainment_train, 
                          distribution = "gaussian", 
                          n.trees = 16 ,
                          shrinkage = 0.1 , 
                          interaction.depth = 7 )

## test set prediction
boosted_tree_model_pred <- predict(boosted_tree_model, newdata = dplyr::select(entertainment_test, -shares), n.trees = 7)

## stores results 
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = entertainment_test$shares)

```

# Comparison

```{r comparison of 4 models }
## creates a data frame of the four models RMSE on the 
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
                        ## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
                         #linear_2_RMSE=linear_2_RMSE[1], 
                         #random_forest_RMSE=random_forest_RMSE[1]
                          boosted_tree_RMSE = boosted_tree_RMSE[1] )

## gets the name of the column with the smallest rmse 
smallest_RMSE<-colnames(models_RMSE)[apply(models_RMSE,1,which.min)]

## declares the model with smallest RSME the winner 
paste0(" For ", 
       ## parameter will be lifestyle for this one I think
       #parameter, 
       smallest_RMSE, " is the winner")

```

# Automation

**not completely sure what we are supposed to do for this. I think it is from topic 2 "Automating R Markdown" where we set the parameters in the yaml and then they will be "lifestyle, bus, tech, etc" and then it will go do everything with param=lifestyle, but let me know if you think there is a better way to do the automation. I will ask him about this 7/3.**
