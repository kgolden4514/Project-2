---
title: "Project 2"
author: "Kristina Golden and Demetrios Samaras"
date: "2023-07-02"
output: html_document
params: 
      data_channel_is: 

---

```{r render with params, echo=FALSE, eval=FALSE, include=FALSE}
## delete ## working on the automation, let me know if you think there is a better way. i think for this we just have to go through at put params$ ## 

## renders with parameters "lifestyle 
rmarkdown::render("/GitHub/ST_558_Project_2", output_file = "Lifestyle.html",
params = list(data_channel_is = "lifestyle"))

## renders all 6 documents 

## creates a list of all 6 desired params from create_dataframes_use_later
X <- c("lifestyle", "entertainment", "business", "socmed", "tech", "world")

## renders with params to all 
sapply(X, FUN = function(X){
  
out <- paste0(X, ".html")
  
rmarkdown::render("/GitHub/ST_558_Project_2", output_file = out,
params = list(data_channel_is = X ))
  
  
})


```




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
setwd("~/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
```

## Required Packages

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
```

## Introduction

**this is just a placeholder for now, will do better when I have a better understanding of what is going on**

## Read in the Data

```{r data, echo=FALSE, message=FALSE, warning=FALSE}
setwd("~/GitHub/ST_558_Project_2")
online <- read.csv('OnlineNewsPopularity.csv')
colnames(online) <- c('url', 'days', 'n.Title', 'n.Content', 'Rate.Unique', 
                      'Rate.Nonstop', 'Rate.Unique.Nonstop', 'n.Links', 
                      'n.Other', 'n.Images', 'n.Videos',
                      'Avg.Words', 'n.Key', 'Lifestyle', 'Entertainment',
                      'Business', 'Social.Media', 'Tech', 'World', 'Min.Worst.Key',
                      'Max.Worst.Key', 'Avg.Worst.Key', 'Min.Best.Key', 
                      'Max.Best.Key', 'Avg.Best.Key', 'Avg.Min.Key', 'Avg.Max.Key',
                      'Avg.Avg.Key', 'Min.Ref', 'Max.Ref', 'Avg.Ref', 'Mon', 
                      'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun', 'Weekend',
                      'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 
                      'Global.Subj', 'Global.Pol', 'Global.Pos.Rate',
                      'Global.Neg.Rate', 'Rate.Pos', 'Rate.Neg', 'Avg.Pos.Pol',
                      'Min.Pos.Pol', 'Max.Pos.Pol', 'Avg.Neg.Pol', 'Min.Neg.Pol',
                      'Max.Neg.Pol', 'Title.Subj', 'Title.Pol', 'Abs.Subj',
                      'Abs.Pol', 'shares')
#Dropped url and timedelta because they are non-predictive. 
online <- online[ , c(3:61)]

#All trained data sets are in another .Rmd file called Create_dataframes_use_later. These can be copy and pasted when they are necessary

## I'm not 100% sure but I think we are supposed to run this same process with the different params set up. so its like just one doc that we knit with the six different params and it goes through and does everything without us needing to copy and paste 
```

# Write Functions

```{r summary_table}
#Creates Summary Tables for a training dataset
summary_table <- function(data_input) {
  d <- describe(data_input[ , c('shares')], fast=TRUE)
  kable(d, caption = 'Shares Summary')
}
```

```{r correlation_table}
#Create correlation table and graph for a training dataset
correlation_table <- function(data_input) {
  #drop binary variables
  correlations <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
  kable(correlations, caption = 'Correlations Lifestyle')
}
```

```{r correlation_graph}
# Create correlation graph
correlation_graph <- function(data_input,sig=0.5){
  corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
  corr[lower.tri(corr, diag = TRUE)] <- NA
  corr <- melt(corr, na.rm = TRUE)
  corr <- subset(corr, abs(value) > 0.5)
  corr[order(-abs(corr$value)),]
  print(corr)
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
```

```{r scatterplots_WIP}
# Still working this one out
# Create scatterplots
# scatterplots <- function(data_input) {
#   
# }
# scatterplots(lifestyle_train)
```

```{r scaterplots testing, include=FALSE, echo=FALSE, eval=FALSE}
## delete ## just looking at the way scatter plots look 

g <-ggplot(lifestyle_train,  aes(y=shares, x= num_imgs))

g+ geom_point()




```

```{r random_forest}
random_forest <- function(data_input){
  set.seed(123)
  fit_control <- trainControl(method='repeatedcv', number=3, repeats=1)
  forest <- train(shares ~., 
                  data = data_input, 
                  method ='rf',
                  ntree = 200,
                  trControl = fit_control)
  var_imp <- varImp(forest, scale=FALSE)$importance
  var_imp <- data.frame(variables=row.names(var_imp), importance=var_imp$Overall)
  a <- var_imp %>%
        arrange(importance) %>%
        ggplot(aes(x=reorder(variables, importance), y=importance)) + 
        geom_bar(stat='identity') + coord_flip() + xlab('Variables') +
        labs(title='Random forest variable importance') + 
        theme_minimal() + 
        theme(axis.text = element_text(size = 10), 
              axis.title = element_text(size = 15), 
              plot.title = element_text(size = 20))
  b <- forest$finalModel
  c <- forest$finalModel$importance
  out <- list(a, b, c)
  return(out)
}
```

# Lifestyle

## Lifestyle EDA

```{r Lifestyle}
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
lifestyle <- online[ , -c(13:17)] 


#Filter out the zeros from the remaining data_channel_is column
lifestyle <- lifestyle %>%
             filter(Lifestyle == 1)

#Drop the data_channel_is column
lifestyle <- lifestyle[ , -c(12)] 
lifestyle <- lifestyle[ , c(53, 1:52)]
```

```{r lifestyle_train_test}
#Split the data into training and test
set.seed(5432)

# Split the data into a training and test set (70/30 split)
# indices

train <- sample(1:nrow(lifestyle), size = nrow(lifestyle)*.70)
test <- setdiff(1:nrow(lifestyle), train)

# trainiing and testing subsets
lifestyle_train <- lifestyle[train, ]
lifestyle_test <- lifestyle[test, ]
```

```{r lifestyle_summary}
#Shares table for lifestyle_train
summary_table(lifestyle_train)
```

```{r lifestyle_corr_table}
#Correlation table for lifestyle_train
correlation_table(lifestyle_train)
```

```{r lifestyle_corr_graph}
#Correlation graph for lifestyle_train
correlation_graph(lifestyle_train)
```

# Entertainment

## Entertainment EDA

```{r entertainment}
#Create data_channel_is_lifestyle dataset
#Take out all data_Channel_is columns that are not relevant to this data set
entertainment <- online[ , -c(12, 14:17)]

#Filter out the zeros from the remaining data_channel_is column
entertainment <- entertainment %>%
                 filter(Entertainment == 1)

#Drop the data_channel_is column
entertainment <- entertainment[ , -c(12)]
entertainment <- entertainment[ , c(53, 1:52)]
```

```{r entertainment_train_test}
#Split the data into training and test
set.seed(5432)

# Split the data into a training and test set (70/30 split)
# indices

train <- sample(1:nrow(entertainment), size = nrow(entertainment)*.70)
test <- setdiff(1:nrow(entertainment), train)

# trainiing and testing subsets
entertainment_train <- entertainment[train, ]
entertainment_test <- entertainment[test, ]
```

# Modeling

## Linear regression 1

```{r Linear regression 1 }
## linear regression model using all predictors 
linear_model_1 <- train( shares ~ ., data = lifestyle_train,
                         method = "lm",
                         preProcess = c("center", "scale"),
                         trControl = trainControl(method = "cv", 
                                                  number = 5))

## prediction of test with model 
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(lifestyle_test, -shares))

## storing error of model on test set 
linear_1_RMSE<- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)

```

## Linear regression 2

```{r Linear regression 2 }
## delete ## I think we can just copy and paste linear regression 1 with slightly different predictors after the ~ ?  let me know what you think. ##

```


## Random forest model

```{r random_forest}

random_forest(lifestyle_train)
#I have not learned multiple regression etc in my statistics courses yet. I have a very general idea of what topic 3 is all about. I am doing my best though! 
```

## Boosted tree model

```{r boosted tree tuning, cache=TRUE}

## cross validation on boosted tree method to find optima tuning parameters 
## DELETE after ## I'm really confused as to why higher values are not giving better rsme for tuning I'm going to go to office hours 7/3 to try to figure out. will use random parameters for now  ## 

## creates grid of possible tuning parameters 
gbm_grid <-  expand.grid(interaction.depth = c(1:7), 
  n.trees = (1:10)*50, 
  shrinkage = 0.1,
  n.minobsinnode = 20)

## sets trainControl method 
fit_control <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats= 1)

set.seed(13)

## trains to find optimal tuning parameters except it is giving weird parameters 
gbm_tree_cv <- train(shares ~ ., data = lifestyle_train,
                     method = "gbm",
                     preProcess = c("center", "scale"),
                     trControl = fit_control,
                     tuneGrid= gbm_grid,
                     verbose=FALSE)
## plot to visualize parameters 
plot(gbm_tree_cv)

```

```{r boosted tree tuned}
## creates boosted tree model with tuning parameters n.trees, shrinkage, interaction.depth from CV 
boosted_tree_model <- gbm(shares ~ ., data = lifestyle_train, 
                          distribution = "gaussian", 
                          n.trees = 500 ,
                          shrinkage = 0.1 , 
                          interaction.depth = 6 )

## test set prediction
boosted_tree_model_pred <- predict(boosted_tree_model, newdata = dplyr::select(lifestyle_test, -shares), n.trees = 500)

## stores results 
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = lifestyle_test$shares)

```

# Comparison

```{r comparison of 4 models }
## creates a data frame of the four models RMSE on the 
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
                        ## delete after ## I can take care of these other two if you want since you did so much of the data stuff just let me know ##
                         #linear_2_RMSE=linear_2_RMSE[1], 
                         #random_forest_RMSE=random_forest_RMSE[1]
                          boosted_tree_RMSE = boosted_tree_RMSE[1] )

## gets the name of the column with the smallest rmse 
smallest_RMSE<-colnames(models_RMSE)[apply(models_RMSE,1,which.min)]

## declares the model with smallest RSME the winner 
paste0(" For ", 
       ## parameter will be lifestyle for this one I think
       #parameter, 
       smallest_RMSE, " is the winner")

```

# Automation

**not completely sure what we are supposed to do for this. I think it is from topic 2 "Automating R Markdown" where we set the parameters in the yaml and then they will be "lifestyle, bus, tech, etc" and then it will go do everything with param=lifestyle, but let me know if you think there is a better way to do the automation. I will ask him about this 7/3.**
