---
title: "Project 2"
author: "Kristina Golden and Demetrios Samaras"
date: "2023-07-02"
output: html_document
params: 
  DataChannel: "Tech"
---

```{r render with params, eval=FALSE, include=FALSE}
## DELETE## This will also only knit htmls, we have to change the output to github_document and use the one at the bottom to get it to generate github documents  

## creates a list of all 6 desired params from online
data_channel_is <- c("Lifestyle", "Entertainment", "Business", "Social.Media", "Tech", "World")

output_file <- paste0(data_channel_is, ".html")
#create a list for each team with just the team name parameter
params = lapply(data_channel_is, FUN = function(x){list(DataChannel = x)})
#put into a data frame
reports <- tibble(output_file, params)

## renders with params to all 
apply(reports, MARGIN=1, FUN = function(x){
  
rmarkdown::render("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2/_Rmd/ST_558_Project_2.Rmd", output_dir = "./automations_test2_html", output_file = x[[1]], params = x[[2]]
    )
  }
)


##Delete## So, I changed all necessary inputs to data_channel_train and data_channel_test. I can change the default YAML and see that it does create do everything for the data channel specified in the YAML. That's awesome! But, it is running all 6 data channels at once? If that is the case, where are the htmls going for each data channel? I see the folder with htmls in it, but the htmls have a bunch of 0s and 1s. 

## DELETe## so i think the issue may be you have to run the actual code chuck and then change the "c:/Users towhatever the path of this file is on your device and then the output_dir to whereever you want the 6 documents to render ( you can see all 6 rendered from running this code chunk in the automations_test folder ). but it wont work if you knit it the normal way. we will also need to make sure it is giuthub_document for the final so it shows up properly 



##About the written commentary, are we supposed to provide that for every data_channel? If so, how are we automating that? We still need to add the factor contingency graphs. Do we want to set a binary to shares like we did for the HW8 for rented bike count? If so, what is your suggestion? I do not mind making the tables for that. As far as the blog post goes, I do not mind writing it. I do have trouble with github in the following ways 1.) Even though I use and .md file, getting my output images to show up in my blog posts, 2.) I do not know how to set a visually appealing theme.

## DELETE## i dont think so. so we just need to write general commentary where the project says "As you will automate this same analysis across other data, you canâ€™t describe the trends you see in the graph (unless you want to try to automate that!)" which i honestly dont want to try to automate that lol and i dont think we get any points for that so unless you really want to automate i would just write general commentary. as far as the contigency tables and graphs i think the project says we are both supposed to make some contigency tables and three graphs, "Each group member is responsible for producing some summary statistics (means, sds, contingency tables, etc.) and for producing at least three graphs (each) of the data."  so for my contigency tables (make sure these are two way tables i asked him and he emphatically said two way tables lol) i just did shares like a factor of above or below the mean shares (called it shareshigh) and then compared it to the weekend and monday and then made a histogram and col graph and scatterplot of some differnt variables. so i would just do something along those lines with whatever variables you think are interesting.  i think we also both have to write out own blog post but like i will use the links to your repo and pages, we dont have to put the actual docs in the blog ( they will be in the repo and the links will be in the readme) . as far as the output i think that may be because the output has to be github_document? im not sure on that one ive never had that issue but i can look into it. i wouldnt worry about a visually appealing theme unless its something you care about, i didnt pick a theme at all for my pages for project 1 and didnt lose any points for it. i think unless its in the grading rubric or notes on grading on the project 2 document then we wont get dinged for it. sorry i wrote so much! i proabably wont be on later today after i finish my commentary but if you have any issues feel free to text me at 7192898738 or email me at dlsamara@ncsu.edu and i will get on (or tomorrow morning depending on the time) and we can make sure everything is working for the submission tomorrow. 


```

```{r setup , include=FALSE}

## DELETE## changed this to eval = FALSE so it would knit on my machine please feel free to change back if i forget
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
setwd("C:/Documents/Github/ST_558_Project_2")
#setwd("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2")
options(scipen = 1, digits = 6)
```

# `r params$DataChannel`

## Required Packages

```{r packages, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(GGally)
library(corrplot)
library(qwraps2)
library(vtable)
library(psych)
library(ggplot2)
library(cowplot)
library(caret)
library(gbm)
library(randomForest)
library(tree)
library(class)
library(bst)
library(reshape)
library(reshape2)
library(corrr)
library(ggcorrplot)
library(FactoMineR)
library(factoextra)
library(data.table)
```

## Introduction

In this report we will be looking at the `r params$DataChannel` data channel of the online news popularity data set. This data set looks at a wide range of variables from 39644 different news articles. The response variable that we will be focusing on is **shares**. The purpose of this analysis is to try to predict how many shares a `r params$DataChannel` article will get based on the values of those other variables. We will be modeling shares using two different linear regression models and two ensemble tree based models.

## Read in the Data


```{r data, echo=TRUE, message=FALSE, warning=FALSE}



setwd("C:/Documents/Github/ST_558_Project_2")
#setwd("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/GitHub/ST_558_Project_2")
##Delete## I learned this trick while working my summer internship. When you are collaborating with people, put your project in your documents in a folder named Github. Everyone collaborating does the same. Then we have the same working directory. I just commented your working directory out so it would run the code for me, while you're working, fill free to comment mine out and uncomment yours. 
## Oh dang that is a good idea haha i will definitly do that next time 

online <- read.csv('OnlineNewsPopularity.csv')
colnames(online) <- c('url', 'days', 'n.Title', 'n.Content', 'Rate.Unique', 
                      'Rate.Nonstop', 'Rate.Unique.Nonstop', 'n.Links', 
                      'n.Other', 'n.Images', 'n.Videos',
                      'Avg.Words', 'n.Key', 'Lifestyle', 'Entertainment',
                      'Business', 'Social.Media', 'Tech', 'World', 'Min.Worst.Key',
                      'Max.Worst.Key', 'Avg.Worst.Key', 'Min.Best.Key', 
                      'Max.Best.Key', 'Avg.Best.Key', 'Avg.Min.Key', 'Avg.Max.Key',
                      'Avg.Avg.Key', 'Min.Ref', 'Max.Ref', 'Avg.Ref', 'Mon', 
                      'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun', 'Weekend',
                      'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 
                      'Global.Subj', 'Global.Pol', 'Global.Pos.Rate',
                      'Global.Neg.Rate', 'Rate.Pos', 'Rate.Neg', 'Avg.Pos.Pol',
                      'Min.Pos.Pol', 'Max.Pos.Pol', 'Avg.Neg.Pol', 'Min.Neg.Pol',
                      'Max.Neg.Pol', 'Title.Subj', 'Title.Pol', 'Abs.Subj',
                      'Abs.Pol', 'shares')
#Dropped url and timedelta because they are non-predictive. 
online <- online[ , c(3:61)]


```

## Write Functions

```{r summary_table}
summary_table <- function(data_input) {
    min <- min(data_input$shares)
    q1 <- quantile(data_input$shares, 0.25)
    med <- median(data_input$shares)
    q3 <- quantile(data_input$shares, 0.75)
    max <- max(data_input$shares)
    mean1 <- mean(data_input$shares)
    sd1 <- sd(data_input$shares)
    Summary <- as.data.frame(cbind(min, q1, med, q3, max,
                                   mean1, sd1))
    colnames(Summary) <- c("Minimum", "Q1", "Median", "Q3",
                           "Maximum", "Mean", "SD")
    rownames(Summary) <- c("Shares")
    setDT(Summary)
    Summary
}
  
```

```{r correlation_table}
#Create correlation table and graph for a training dataset
correlation_table <- function(data_input) {
  #drop binary variables
  correlations <- cor(subset(data_input, select = c(2:4, 6:24,
                                                    33:50)))
  kable(correlations, caption = 'Correlations Lifestyle')
}

```

```{r correlation_graph}
# Create correlation graph
correlation_graph <- function(data_input,sig=0.5){
  corr <- cor(subset(data_input, select = c(2:4, 6:24, 33:50)))
  corr[lower.tri(corr, diag = TRUE)] <- NA
  corr <- melt(corr, na.rm = TRUE)
  corr <- subset(corr, abs(value) > 0.5)
  corr[order(-abs(corr$value)),]
  print(corr)
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="value")
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
```

```{r scatterplots_WIP}
# Still working this one out
# Create scatterplots
# scatterplots <- function(data_input) {
#   
# }
# scatterplots(lifestyle_train)
```


## `r params$DataChannel` EDA



` 

### `r params$DataChannel` 


```{r automation creation of each data set, message=FALSE }

## filters rows based on when parameter is 1 
data_channel <-  online %>% filter( !!rlang::sym(params$DataChannel) == 1)

## Drop the data_channel_is columns 
data_channel <- data_channel[ , -c(12:17)]

## reorder to put shares first 
data_channel <- data_channel[ , c(53, 1:52)]

```

```{r automation train and test }

set.seed(5432)

# Split the data into a training and test set (70/30 split)
# indices

train <- sample(1:nrow(data_channel), size = nrow(data_channel)*.70)
test <- setdiff(1:nrow(data_channel), train)

# training and testing subsets
data_channel_train <- data_channel[train, ]
data_channel_test <- data_channel[test, ]


```

## `r params$DataChannel` Summarizations

```{r r params$DataChannel_summary, echo=TRUE}
summary_table(data_channel_train)
```


```{r params$DataChannel_corr_table}

## this had the lifestyle stuff, i deleted it because we dont want there to be lifestyle in the other data channel documents. 

```

## `r params$DataChannel` Summarizations 

```{r lifestyle_summary}
#Shares table for lifestyle_train
summary_table(data_channel_train)
```

```{r lifestyle_corr_table}

#Correlation table for lifestyle_train
correlation_table(data_channel_train)
```

```{r r params$DataChannel corr_graph, message=FALSE, warning=FALSE}
#Correlation graph for lifestyle_train
correlation_graph(data_channel_train)
```



```{r contingency tables }
## mean of shares 
mean(data_channel_train$shares)

## sd of shares 
sd(data_channel_train$shares)

## creates a new column that is if shares is higher than average or not 
shareshigh <- data_channel_train %>% select(shares) %>% mutate (shareshigh = (shares> mean(shares)))

## creates a contingency table of shareshigh and whether it is the weekend 
table(shareshigh$shareshigh, data_channel_train$Weekend)

## creates  a contingency table of shareshigh and whether it is monday 
table(shareshigh$shareshigh, data_channel_train$Mon)

```

These above contingency tables will look at the shareshigh factor which says whether the number of shares is higher than the mean number of shares or not and compares it to both the weekend and Monday. Using these we can see if the number of shares tends to be higher or not on the weekend and on Monday



```{r shares histogram }
## creates plotting object of shares
a <- ggplot(data_channel_train, aes(x=shares))

## histogram of shares 
a+geom_histogram(color= "red", fill="blue")+ ggtitle("Shares histogram")

```

Above we can see the frequency distribution of the `r params$DataChannel` data channel. We should always see a long tail to the right because a small number of articles will get a very high number of shares. But looking at by looking at the distribution we can say how many shares most of these articles got. 


```{r col graph }
## creates plotting object with number of words in title and shares
b<- ggplot(data_channel_train, aes(x=n.Title, y=shares))

## creates a bar chart with number of words in title and shares 
b+ geom_col(fill="blue")+ ggtitle("Number of words in title vs shares") + labs(x="Number of words in title")



```

In the above graph we are looking at the number of shares based on how many words are in the title of the article. if we see a large peak on at the higher number of words it means for this data channel there were more shares on longer titles, and if we see a peak at smaller number of words then there were more shares on smaller titles. 



```{r correlations between shares and other variables, eval=TRUE }
## makes correlation of every variable with shares 
shares_correlations <- cor(data_channel_train)[1,] %>% sort() 

shares_correlations

```


```{r graph of shares with highest correlated var}
## take the name of the highest correlated variable
highest_cor <-shares_correlations[52]  %>% names()

highest_cor

## creats scatter plot looking at shares vs highest correlated variable
g <-ggplot(data_channel_train,  aes(y=shares, x= data_channel_train[[highest_cor]])) 


g+ geom_point(aes(color=as.factor(Weekend))) +geom_smooth(method = lm) + ggtitle(" Highest correlated variable with shares") + labs(x="Highest correlated variable vs shares", color="Weekend")


```

the above graph looks at the relationship between shares and the variable with the highest correlation for the `r params$DataChannel` data channel, and colored based on whether or not it is the weekend. because this is the most positivly correlated variable we should always see an upward trend butthe more correlated they are the more the dots will fall onto the line of best fit. 


## Modeling

### Linear regression 1

```{r Linear regression 1 }
## linear regression model using all predictors 
set.seed(13)

linear_model_1 <- train( shares ~ ., 
                         data = data_channel_train,
                         method = "lm",
                         preProcess = c("center", "scale"),
                         trControl = trainControl(method = "cv", 
                                                  number = 5))

## prediction of test with model 
linear_model_1_pred <- predict(linear_model_1, newdata = dplyr::select(data_channel_test, -shares))

## storing error of model on test set 
linear_1_RMSE<- postResample(linear_model_1_pred, obs = data_channel_test$shares)

```

### Linear regression 2

```{r Linear regression 2 }
## Delete ## i think this looks good! smart things to cut out, i will try to improve mine. ## 

#Removed rate.Nonstop because it was only 1 and removed the days of the week.
linear_model_2 <- train( shares ~. - Rate.Nonstop - Mon
                         - Tues - Wed - Thurs - Fri - Sat
                         - Sun - Weekend, 
                        data = data_channel_train,
                         method = "lm",
                         preProcess = c("center", 
                                        "scale"),
                         trControl = trainControl(
                           method= "cv", 
                           number = 5))
## prediction of test with model 
linear_model_2_pred <- predict(linear_model_2, newdata = dplyr::select(data_channel_test, -shares))

## storing error of model on test set 
linear_2_RMSE<- postResample(linear_model_2_pred, obs = data_channel_test$shares)

```

### Random forest model

```{r random_forest }
set.seed(10210526)
rfFit <- train(shares ~ ., 
        data = data_channel_train,
        method = "rf",
        trControl = trainControl(method = "cv",
                                        number = 5),
        preProcess = c("center", "scale"),
        tuneGrid = 
          data.frame(mtry = ncol(data_channel_train)/3))
rfFit_pred <- predict(rfFit, newdata = data_channel_test)
rfRMSE<- postResample(rfFit_pred, obs =
                            data_channel_test$shares)

##Delete## I will try again to add a tuneGrid to the random forest tomorrow later today

## Delete## i think it looks good! i will need to rewatch the lecture and try to dig in some more cause im really lost on these ensamble tree based models. i think you chose some good ones to remove.  the only thing is in the project it says "Both models should be chosen using cross-validation" for random forest and boosted tree, so i think the random forest does need cv unless im missing something. Im still trying to figure out what is wrong with my cross validation and a way to take the tuning parameters based on each data_channel_is during the automation process. I think we are close though! 

## Delete## I rewatched the lecture for model fitting using the caret package. I made the appropriate modifications. I tried to use the tuneGrid but the code was trying to run for over 5 minutes and it never went through, so I used the suggested mtry for regression from the random forest lecture.

## delete## So i went through the random forrest stuff again while doing the hw that was due today and my random forrest also took forever to run with the tune grid. i think it just depends on the size of the expand.grid that you give it. we should keep that in mind for when we are doing the final render of the project because it may take forever to render all 6 lol

```

### Boosted tree model

A decision tree makes a binary decision based on the value input. A boosted tree model generates a predictive model based on an ensemble of decision trees where better trees are generated based on the performance of previous trees. Our boosted tree model can be tuned using four different parameters: interaction.depth which defines the complexity of the trees being built, n.trees which defines the number of trees built (number of iterations), shrinkage which dictates the rate at which the algorithm learns, and n.minobsinnode which dictates the number of samples left to allow for a node to split. 

```{r boosted tree tuning }
 

## creates grid of possible tuning parameters 
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7), 
  n.trees = c(1:20) , 
  shrinkage = 0.1,
  n.minobsinnode = c(10,20, 40))

## sets trainControl method 
fit_control <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats= 1)

set.seed(13)

## trains to find optimal tuning parameters except it is giving weird parameters 
gbm_tree_cv <- train(shares ~ . , data = data_channel_train,
                     method = "gbm",
                     preProcess = c("center", "scale"),
                     trControl = fit_control,
                     tuneGrid= gbm_grid,
                     verbose=FALSE)
## plot to visualize parameters 
plot(gbm_tree_cv)

## test set prediction
boosted_tree_model_pred <- predict(gbm_tree_cv, newdata = dplyr::select(data_channel_test, -shares), n.trees = 7)

## stores results 
boosted_tree_RMSE <- postResample(boosted_tree_model_pred, obs = data_channel_test$shares)

```

## Comparison

```{r comparison of 4 models }
## creates a data frame of the four models RMSE on the 
models_RMSE <- data.frame(linear_1_RMSE=linear_1_RMSE[1],
                         linear_2_RMSE=linear_2_RMSE[1], 
                         rfRMSE=rfRMSE[1],
                          boosted_tree_RMSE =
                           boosted_tree_RMSE[1] )

models_RMSE

## gets the name of the column with the smallest rmse 
smallest_RMSE<-colnames(models_RMSE)[apply(models_RMSE,1,which.min)]

## declares the model with smallest RSME the winner 
paste0(" For ", 
        params$DataChannel, " ", 
       smallest_RMSE, " is the winner")

```

## Automation


This is the code used to automate the rendering of each document based on the parameter of data_channel_is designated in the YAML. 


```{r render with params code, echo=TRUE, eval=FALSE}

## creates a list of all 6 desired params from online
data_channel_is <- c("Lifestyle", "Entertainment", "Business", "Social.Media", "Tech", "World")

## creates the output file name 
output_file <- paste0(data_channel_is, ".md")

#create a list for each channel with just the channel name parameter
params = lapply(data_channel_is, FUN = function(x){list(DataChannel = x)})

#put into a data frame
reports <- tibble(output_file, params)

## renders with params to all based on rows in reports
apply(reports, MARGIN=1, FUN = function(x){
## change first path to wherever yours is and output_dir to whatever folder you want it to output to   
rmarkdown::render('C:/Documents/Github/ST_558_Project_2/_Rmd/ST_558_project_2', output_dir = "./automations_test_html", output_file = x[[1]], params = x[[2]]
    )
  }
)

```
